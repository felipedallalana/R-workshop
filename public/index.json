[{"authors":["admin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"0758406b69facda86eb2b9110c52787c","permalink":"/authors/felipe/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/felipe/","section":"authors","summary":"","tags":null,"title":"Felipe Dalla Lana","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"1bdbe4c3bbc7a01f493d49ade9f9379b","permalink":"/authors/paul/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/paul/","section":"authors","summary":"","tags":null,"title":"Paul Esker","type":"authors"},{"authors":["Paul Esker and Felipe Dalla Lana"],"categories":["R"],"content":"\rBackground\rThis example is focued on modeling via linear regression. We will illustrate the concepts using an example, with particular focus on the assumptions and the tools that exist in R to explore the model fit.\nOur goal is to related a “dependent variable” with an “independent variable” the explains something about the process.\nOur simple example is that we might relate plant height with an index of crop growth (leaf area index). This would provide a simple base for considering in the future the impact of some pest on growth and development.\nOur basic model form is:\n\\[Y = f(X) + e\\]\nWhere:\n\rY = dependent variable,\rf(X) = a mathematical function that describes the relationship of the dependenct variable as a function of the independent variable,\re = error, the proper form for a model depends on the type of assumptions; in our simple example, we assume that the error is distributed normally with an expected value of 0 and variance equal to sigma.\r\rFor this first example, we are creating a more complete analysis where we will explore some of the tools that help with understanding the model assumptions and also how to use the prediction function, which is important for using the model to estimate new values, as well as information about the variability.\nlibrary(tidyverse)\rlibrary(Hmisc)\rlibrary(corrplot)\rlibrary(readr)\rlibrary(HH)\rlibrary(car)\rlibrary(tinytex)\r\rData\rIn the majority of our examples, we will use a manual data input approach, to minimize some of the confusion that occurs when trying to import data. R and RStudio are very flexible in this regards.\nThe data we are using for this first example comes from peanut, where we have two measures:\r1. The percentage of clean grain,\r2. The concentration of alfatoxin in ppb (ug per kg).\nWe describe the variables as follows:\n\rclean = % of clean grain\n\ralfatoxin = alfatoxin concentration\r\rclean \u0026lt;- c(99.97, 99.94, 99.86, 99.98, 99.93, 99.81, 99.98, 99.91, 99.88, 99.97, 99.97, 99.8, 99.96, 99.99, 99.86, 99.96, 99.93, 99.79, 99.96, 99.86, 99.82, 99.97, 99.99, 99.83, 99.89, 99.96, 99.72, 99.96, 99.91, 99.64, 99.98, 99.86, 99.66, 99.98)\ralfatoxin \u0026lt;- c(3, 18.8, 46.8, 4.7, 18.9, 46.8, 8.3, 21.7, 58.1, 9.3, 21.9, 62.3, 9.9, 22.8, 70.6, 11, 24.2, 71.1, 12.3, 25.8, 71.3, 12.5, 30.6, 83.2, 12.6, 36.2, 83.6, 15.9, 39.8, 99.5, 16.7, 44.3, 111.2, 18.8)\rpeanut \u0026lt;- data.frame(clean, alfatoxin)\rhead(peanut)\r## clean alfatoxin\r## 1 99.97 3.0\r## 2 99.94 18.8\r## 3 99.86 46.8\r## 4 99.98 4.7\r## 5 99.93 18.9\r## 6 99.81 46.8\r\rExploratory analysis\rmean(alfatoxin)\r## [1] 36.60294\rsd(alfatoxin)\r## [1] 29.3194\rsd(alfatoxin)/mean(alfatoxin)*100\r## [1] 80.1012\rmean(clean)\r## [1] 99.89647\rsd(clean)\r## [1] 0.09351332\rsd(clean)/mean(clean)*100\r## [1] 0.09361024\rcor(clean, alfatoxin)\r## [1] -0.9069581\rrcorr(clean, alfatoxin)\r## x y\r## x 1.00 -0.91\r## y -0.91 1.00\r## ## n= 34 ## ## ## P\r## x y ## x 0\r## y 0\r\rLinear regression\r# Visualizing the relationship\rwith(peanut, plot(x=clean, y=alfatoxin, xlim=c(99.5,100), ylim=c(0,120), pch=10)) \r# We will use lm() = linear model, to run the regression\r#Format, Y \u0026lt;- X\rlinreg \u0026lt;- with(peanut, lm(alfatoxin~clean)) #ANOVA table to see how the model fit looks\ranova(linreg)\r## Analysis of Variance Table\r## ## Response: alfatoxin\r## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## clean 1 23334.5 23334.5 148.36 1.479e-13 ***\r## Residuals 32 5033.2 157.3 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r#Another way to see results of the model, with a few more details. This is important as we extend on the modeling concept to understand more complex relationships. summary(linreg)\r## ## Call:\r## lm(formula = alfatoxin ~ clean)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -25.843 -7.997 -2.771 6.835 27.695 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 28443.18 2332.21 12.20 1.43e-13 ***\r## clean -284.36 23.35 -12.18 1.48e-13 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 12.54 on 32 degrees of freedom\r## Multiple R-squared: 0.8226, Adjusted R-squared: 0.817 ## F-statistic: 148.4 on 1 and 32 DF, p-value: 1.479e-13\rThe results indicated that there is a “significant” relationship. In the next step, we are going to learn about some of the tools that we can use to extract more information about the results to look at hypothesis testing on the parameters (intercept, slope, etc.)\n### Example: let\u0026#39;s say that we are interested in comparing the slope to a known value of -220, which means that for every 1% change in the percentage of clean grain, the concentration of alfatoxin will be reduced by 220 ug per kg\r# First, we need to see and understand where the coefficients are located, especially the intercept and slope\rlinreg$coef\r## (Intercept) clean ## 28443.1778 -284.3601\rlinreg$coef[1]\r## (Intercept) ## 28443.18\rlinreg$coef[2]\r## clean ## -284.3601\r# Furthermore, where are the errors associated with each parameter\rcoefs \u0026lt;- summary(linreg)\rnames(coefs)\r## [1] \u0026quot;call\u0026quot; \u0026quot;terms\u0026quot; \u0026quot;residuals\u0026quot; \u0026quot;coefficients\u0026quot; \u0026quot;aliased\u0026quot; \u0026quot;sigma\u0026quot; \u0026quot;df\u0026quot; \u0026quot;r.squared\u0026quot; \u0026quot;adj.r.squared\u0026quot; \u0026quot;fstatistic\u0026quot; \u0026quot;cov.unscaled\u0026quot;\rcoefs$coefficients\r## Estimate Std. Error t value Pr(\u0026gt;|t|)\r## (Intercept) 28443.1778 2332.20556 12.19583 1.429478e-13\r## clean -284.3601 23.34622 -12.18014 1.479070e-13\r# We can see this directly as follows: coefs$coefficients[1,1]\r## [1] 28443.18\rcoefs$coefficients[1,2]\r## [1] 2332.206\rcoefs$coefficients[2,1]\r## [1] -284.3601\rcoefs$coefficients[2,2]\r## [1] 23.34622\r# Now, we will define the test parameter value for the slope\rB1 \u0026lt;- -220\r# To realize the test, we need to define the parameter value and the appropriate error term # abs = absolute value\rtest_b1\u0026lt;-abs((coefs$coefficients[2,1]-B1)/coefs$coefficients[2,2])\rtest_b1\r## [1] 2.75677\r## Test statistic (two-tailed) with 32 degrees of freedom (error term) 2*pt(q=test_b1, df=32, lower.tail=FALSE) \r## [1] 0.009560549\r\rModel assumptions\r## What does a simple call to plot provide?\rplot(linreg)\r## With Rmarkdown and the reporting tools, we may have interest in controlling the outputted graphics, which can be accomplished as follows:\rpar(mfrow=c(1,1))\rplot(linreg, which=1)\rplot(linreg, which=2)\rplot(linreg, which=3)\rplot(linreg, which=4)\rplot(linreg, which=5)\rplot(linreg, which=6)\r\rEstimation and prediction\rNow that we have a model, we are normally interested in performing some type of prediction based on the model equation (form). In R, the function predict() is very important for many of the modeling tools we might like to apply. This versatile function allows us to perform estimation (within the confines of the model and data structure) and prediction (under uncertainty). What this predicts is the point estimate for a value (or estiamtes for multiple values) as well as the respective interval type (confidence or prediction).\n# One challenge with predict is the need to defien a data.frame, even if just for a single value, like the following example where the % clean grain is 99.68. observation \u0026lt;- data.frame(clean=99.68)\rpredict(object=linreg, newdata=observation, interval=\u0026quot;confidence\u0026quot;)\r## fit lwr upr\r## 1 98.15855 86.97085 109.3462\rpredict(object=linreg, newdata=observation, interval=\u0026quot;predict\u0026quot;)\r## fit lwr upr\r## 1 98.15855 70.27011 126.047\r# We can do the same for all values in the regression. intervals\u0026lt;-predict(linreg, interval=\u0026quot;confidence\u0026quot;)\rintervals\r## fit lwr upr\r## 1 15.69411 10.088679 21.29954\r## 2 24.22491 19.379382 29.07044\r## 3 46.97372 42.261813 51.68563\r## 4 12.85051 6.936739 18.76427\r## 5 27.06851 22.406270 31.73076\r## 6 61.19173 55.183124 67.20034\r## 7 12.85051 6.936739 18.76427\r## 8 32.75572 28.327614 37.18382\r## 9 41.28652 36.835944 45.73710\r## 10 15.69411 10.088679 21.29954\r## 11 15.69411 10.088679 21.29954\r## 12 64.03533 57.691794 70.37887\r## 13 18.53771 13.215931 23.85949\r## 14 10.00690 3.763770 16.25004\r## 15 46.97372 42.261813 51.68563\r## 16 18.53771 13.215931 23.85949\r## 17 27.06851 22.406270 31.73076\r## 18 66.87893 60.183421 73.57445\r## 19 18.53771 13.215931 23.85949\r## 20 46.97372 42.261813 51.68563\r## 21 58.34813 52.654402 64.04186\r## 22 15.69411 10.088679 21.29954\r## 23 10.00690 3.763770 16.25004\r## 24 55.50453 50.102122 60.90693\r## 25 38.44292 34.051014 42.83482\r## 26 18.53771 13.215931 23.85949\r## 27 86.78414 77.317367 96.25092\r## 28 18.53771 13.215931 23.85949\r## 29 32.75572 28.327614 37.18382\r## 30 109.53295 96.573565 122.49234\r## 31 12.85051 6.936739 18.76427\r## 32 46.97372 42.261813 51.68563\r## 33 103.84575 91.777175 115.91433\r## 34 12.85051 6.936739 18.76427\rpredictions\u0026lt;-predict(linreg, interval=\u0026quot;predict\u0026quot;)\r## Warning in predict.lm(linreg, interval = \u0026quot;predict\u0026quot;): predictions on current data refer to _future_ responses\rpredictions\r## fit lwr upr\r## 1 15.69411 -10.459699 41.84791\r## 2 24.22491 -1.776625 50.22645\r## 3 46.97372 20.996756 72.95069\r## 4 12.85051 -13.371114 39.07213\r## 5 27.06851 1.100509 53.03652\r## 6 61.19173 34.948558 87.43490\r## 7 12.85051 -13.371114 39.07213\r## 8 32.75572 6.828726 58.68271\r## 9 41.28652 15.355682 67.21736\r## 10 15.69411 -10.459699 41.84791\r## 11 15.69411 -10.459699 41.84791\r## 12 64.03533 37.713455 90.35721\r## 13 18.53771 -7.556774 44.63219\r## 14 10.00690 -16.290956 36.30476\r## 15 46.97372 20.996756 72.95069\r## 16 18.53771 -7.556774 44.63219\r## 17 27.06851 1.100509 53.03652\r## 18 66.87893 40.470022 93.28784\r## 19 18.53771 -7.556774 44.63219\r## 20 46.97372 20.996756 72.95069\r## 21 58.34813 32.175256 84.52100\r## 22 15.69411 -10.459699 41.84791\r## 23 10.00690 -16.290956 36.30476\r## 24 55.50453 29.393482 81.61557\r## 25 38.44292 12.522086 64.36375\r## 26 18.53771 -7.556774 44.63219\r## 27 86.78414 59.540418 114.02787\r## 28 18.53771 -7.556774 44.63219\r## 29 32.75572 6.828726 58.68271\r## 30 109.53295 80.887772 138.17814\r## 31 12.85051 -13.371114 39.07213\r## 32 46.97372 20.996756 72.95069\r## 33 103.84575 75.592411 132.09909\r## 34 12.85051 -13.371114 39.07213\r# If we are interested in just some select values, it is easy to accomplish this going back to the original single value example:\robservations \u0026lt;- data.frame(clean=c(99.5, 99.6, 99.7, 99.8))\rpredict(object=linreg, newdata=observations, interval=\u0026quot;confidence\u0026quot;)\r## fit lwr upr\r## 1 149.34338 129.98701 168.69974\r## 2 120.90736 106.14377 135.67095\r## 3 92.47135 82.15206 102.79063\r## 4 64.03533 57.69179 70.37887\rpredict(object=linreg, newdata=observations, interval=\u0026quot;predict\u0026quot;)\r## fit lwr upr\r## 1 149.34338 117.29233 181.39442\r## 2 120.90736 91.40203 150.41269\r## 3 92.47135 64.91979 120.02290\r## 4 64.03533 37.71345 90.35721\r\rAdditional material\rThe package HH (Statistical analysis and data display) has various (interesting) functions that we can use to examine a regression model. In the next section, we will look at several of those.\n# Let\u0026#39;s examine the regression graphically\rci.plot(linreg)\r# Tools to study the assumptions\r# Method to look for outliers using a Bonferroni adjustment\routlierTest(linreg) \r## No Studentized residuals with Bonferroni p \u0026lt; 0.05\r## Largest |rstudent|:\r## rstudent unadjusted p-value Bonferroni p\r## 24 2.425727 0.021292 0.72394\r# Quantile-quantile plot based on Student residuals\rqqPlot(linreg) \r## [1] 24 25\r# Influence plot in which the size of the circle is proportion to Cook\u0026#39;s distance\rinfluencePlot(linreg) \r## StudRes Hat CookD\r## 24 2.4257274 0.04472257 0.11949821\r## 25 -2.2158610 0.02955685 0.06663113\r## 30 -0.9262390 0.25734844 0.14930872\r## 33 0.6594215 0.22318480 0.06358898\r# Test of homoscedasticity ncvTest(linreg)\r## Non-constant Variance Score Test ## Variance formula: ~ fitted.values ## Chisquare = 0.183475, Df = 1, p = 0.6684\r# Method to verify if there is dependency in the model, which means that a transformation may be appropriate to model the relationship\rspreadLevelPlot(linreg) \r## ## Suggested power transformation: 0.9466765\r# Method to verify if there is evidence that the relationship is not linear\rcrPlots(linreg)\r\rSummary\rIn this exercise, the goal was to introduce different concepts in modeling, using a simple linear regression. With this base, we will extend the modeling idea with different examples that illustrate some of the tools that exist in R when we have more complex relationships. Given the time available for this workshop, even if the subsequent examples are more difficult to understand, this first, more developed example hopefully provides you some of the relevant tools to take the next step in your work to define and use different models. .\n\rExample\rThe below example looks at the relationship between the weight of chickens as a function of the amount of lysine, which is an essential amino acid in the early phases of development.\nweight \u0026lt;-c(14.7, 17.8, 19.6, 18.4, 20.5, 21.1, 17.2, 18.7, 20.2, 16.0, 17.8, 19.4)\rlysine \u0026lt;-c(0.09, 0.14, 0.18, 0.15, 0.16, 0.23, 0.11, 0.19, 0.23, 0.13, 0.17, 0.21)\r\r","date":1564711690,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564711690,"objectID":"92f13f53e4a4b2d4e2f5cf78b9b34536","permalink":"/post/linear_regression/","publishdate":"2019-08-01T21:08:10-05:00","relpermalink":"/post/linear_regression/","section":"post","summary":"Background\rThis example is focued on modeling via linear regression. We will illustrate the concepts using an example, with particular focus on the assumptions and the tools that exist in R to explore the model fit.\nOur goal is to related a “dependent variable” with an “independent variable” the explains something about the process.\nOur simple example is that we might relate plant height with an index of crop growth (leaf area index).","tags":["Regression"],"title":"Regression 1","type":"post"},{"authors":["Paul Esker and Felipe Dalla Lana"],"categories":["R"],"content":"\rBackground\rWhen building a model, there are different methods we can take to construct it, ranging from manual to automated. There are strengths and weaknesses in using the different methods, but they provide a good background for those interested in taking their models to a higher level (machine level, etc.), since in those situations we are often interested to look for interactions that cannot easily be found with basic approaches.\nThe general idea in this example is that we are interested in examining the behaviour of the a dependent variable \\(Y\\) as a function of different (possible) explanatory variables, \\(X_i\\). The question that we are asking by looking at the different models is, “Is there a best approach to examing the different relationships?”\nWe will examine different approaches in this exercise and be prepared that the final model may not be the same (we will see a different example after that provides a “cleaner” model, if you will).\nlibrary(tidyverse)\rlibrary(Hmisc)\rlibrary(corrplot)\rlibrary(readr)\rlibrary(HH)\rlibrary(car)\rlibrary(scatterplot3d)\rlibrary(leaps)\rlibrary(olsrr)\r\rData\rFor this exercise, we will examine the relationshiop between the number of aphids in 34 lots as a function of temperature and relative humidity.\nlot \u0026lt;- c(1:34)\raphids \u0026lt;- c(61, 77, 87, 93, 98, 100, 104, 118, 102, 74, 63, 43, 27, 19, 14, 23, 30, 25, 67, 40, 6, 21, 18, 23, 42, 56, 60, 59, 82, 89, 77, 102, 108, 97)\rtemperature \u0026lt;- c(21, 24.8, 28.3, 26, 27.5, 27.1, 26.8, 29, 28.3, 34, 30.5, 28.3, 30.8, 31, 33.6, 31.8, 31.3, 33.5, 33, 34.5, 34.3, 34.3, 33, 26.5, 32, 27.3, 27.8, 25.8, 25, 18.5, 26, 19, 18, 16.3)\rhumidity \u0026lt;- c(57,48, 41.5, 56, 58, 31, 36.5, 41, 40, 25, 34, 13, 37, 19, 20, 17, 21, 18.5, 24.5, 16, 6, 26, 21, 26, 28, 24.5, 39, 29, 41, 53.5, 51, 48, 70, 79.5)\raphids_data \u0026lt;- data.frame(lot, aphids, temperature, humidity)\r\rBasic model\rOur basic model is additive, meaning we expect there to be an effect of temperature and relative humidity:\n\\[aphids = intercept + temperature + humidity + error\\]\nOur model structure will start by assuming that both temperature and humidity explain “something” about the relationship. For completeness, one could start with each factor separately and examine the explanatory value and then build the subsequent model accordingly. In many situations, what we are most interested in understanding is if there are interactions that explain better the relationships, especially if it is not clear that a linear set of assumptions is appropriate. We will build on those ideas in subsequent steps.\nmodel_a \u0026lt;- with(aphids_data, lm(aphids ~ temperature + humidity))\ranova(model_a) # both factors are significant\r## Analysis of Variance Table\r## ## Response: aphids\r## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## temperature 1 15194.8 15194.8 28.7765 7.554e-06 ***\r## humidity 1 4813.1 4813.1 9.1151 0.005038 ** ## Residuals 31 16368.9 528.0 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rsummary(model_a) #R^2 = 0.55\r## ## Call:\r## lm(formula = aphids ~ temperature + humidity)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -35.393 -14.006 -3.198 10.335 49.265 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 35.8255 53.5388 0.669 0.50835 ## temperature -0.6765 1.4360 -0.471 0.64089 ## humidity 1.2811 0.4243 3.019 0.00504 **\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 22.98 on 31 degrees of freedom\r## Multiple R-squared: 0.55, Adjusted R-squared: 0.521 ## F-statistic: 18.95 on 2 and 31 DF, p-value: 4.212e-06\rplot(model_a)\rplot(model_a, which=4) \r\rFull model\rThe second model we will build takes into account what we define to be the full model. In some situations, this may just be all factors and all interactions. Here, given the two potential explanatory factors and the idea that there may not be purely a linear relationship, we will build our full model based on the individual factors, the interaction between those factors, as well as a quadratic form for the model for each factor.\nModel B: \\(aphids = intercept + temperature + humidity + temperature^2 + humidity^2 + temperature:humidity\\)\nWe will use an indicator function to define the quadratic model forms (\\(I\\)) in the subsequent model.\nmodel_b \u0026lt;- with(aphids_data, lm(aphids ~ temperature + humidity + I(temperature^2) + I(humidity^2) + temperature:humidity))\ranova(model_b) # significant factors: temperature, humidity, I(temperature^2)\r## Analysis of Variance Table\r## ## Response: aphids\r## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## temperature 1 15194.8 15194.8 31.4527 5.278e-06 ***\r## humidity 1 4813.1 4813.1 9.9629 0.003801 ** ## I(temperature^2) 1 1982.4 1982.4 4.1036 0.052418 . ## I(humidity^2) 1 805.1 805.1 1.6666 0.207279 ## temperature:humidity 1 54.6 54.6 0.1129 0.739344 ## Residuals 28 13526.8 483.1 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rsummary(model_b) #R^2 = 0.63, adjusted R^2 = 0.5617\r## ## Call:\r## lm(formula = aphids ~ temperature + humidity + I(temperature^2) + ## I(humidity^2) + temperature:humidity)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -41.700 -12.220 -1.462 10.894 41.673 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|)\r## (Intercept) 143.069144 610.542500 0.234 0.816\r## temperature -5.639044 33.900957 -0.166 0.869\r## humidity -0.182206 8.875236 -0.021 0.984\r## I(temperature^2) 0.029174 0.476345 0.061 0.952\r## I(humidity^2) -0.008121 0.036214 -0.224 0.824\r## temperature:humidity 0.078534 0.233701 0.336 0.739\r## ## Residual standard error: 21.98 on 28 degrees of freedom\r## Multiple R-squared: 0.6281, Adjusted R-squared: 0.5617 ## F-statistic: 9.46 on 5 and 28 DF, p-value: 2.285e-05\rplot(model_b)\rplot(model_b, which=4)\r\rModel comparison\r# anova(a,b), enables comparision between nested models, based on the number of additional parameters\ranova(model_a, model_b) \r## Analysis of Variance Table\r## ## Model 1: aphids ~ temperature + humidity\r## Model 2: aphids ~ temperature + humidity + I(temperature^2) + I(humidity^2) + ## temperature:humidity\r## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F)\r## 1 31 16369 ## 2 28 13527 3 2842.1 1.961 0.1427\r# What the results indicates is that a full model does not explain better the relationship, probably due to being an over-adjusted model. This does not mean that they may not be a model that better explains the relationship that is less complicated. \r\rModeling: three methods under consideration\rNow, we will look at different methods to try to automate the model development. The general idea with this approach/exercise is to reduce the need to create many models and duplicate the same process over and over. The challenge will be to identify the most important factors, not just statistically, but also based on the biology and knowledge of the system.\nThe three methods we will consider:\r* Manual\r* Stepwise methods (“steps”)\r* Best subset methods\n# Manual model construction\r# We will start with Model B in this situation and try to reduce the complexity of the model. # The process involves elminating factors that are not significant followed by an examination of the new model fit.\r# We assume that we will work from interactions towards the simple, single factor models.\r# Initial step: eliminate the factor, temperature:humidity\rmodel_b2 \u0026lt;- update(model_b, .~.-temperature:humidity)\rsummary(model_b2)\r## ## Call:\r## lm(formula = aphids ~ temperature + humidity + I(temperature^2) + ## I(humidity^2))\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -40.969 -12.837 -1.096 12.670 41.617 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -57.45005 127.24411 -0.451 0.6550 ## temperature 5.24487 9.85861 0.532 0.5988 ## humidity 2.77322 1.17389 2.362 0.0251 *\r## I(temperature^2) -0.11851 0.18088 -0.655 0.5175 ## I(humidity^2) -0.01921 0.01465 -1.311 0.2001 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 21.64 on 29 degrees of freedom\r## Multiple R-squared: 0.6266, Adjusted R-squared: 0.5752 ## F-statistic: 12.17 on 4 and 29 DF, p-value: 6.302e-06\ranova(model_b, model_b2) ## temperature:humidity = non-significant\r## Analysis of Variance Table\r## ## Model 1: aphids ~ temperature + humidity + I(temperature^2) + I(humidity^2) + ## temperature:humidity\r## Model 2: aphids ~ temperature + humidity + I(temperature^2) + I(humidity^2)\r## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F)\r## 1 28 13527 ## 2 29 13581 -1 -54.554 0.1129 0.7393\r# From model b2, we will now eliminate the factor I(humidity^2)\rmodel_b3 \u0026lt;- update(model_b2, .~.-I(humidity^2))\rsummary(model_b3) # it appears that all factors explain something\r## ## Call:\r## lm(formula = aphids ~ temperature + humidity + I(temperature^2))\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -37.782 -13.595 -3.561 9.367 43.291 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -152.5753 105.7801 -1.442 0.15955 ## temperature 13.9936 7.3439 1.905 0.06634 . ## humidity 1.3263 0.4050 3.275 0.00267 **\r## I(temperature^2) -0.2769 0.1362 -2.033 0.05096 . ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 21.9 on 30 degrees of freedom\r## Multiple R-squared: 0.6045, Adjusted R-squared: 0.565 ## F-statistic: 15.29 on 3 and 30 DF, p-value: 3.217e-06\r# Compare the models b2 and b3\ranova(model_b2, model_b3) # I(humidity^2) = not signficant \r## Analysis of Variance Table\r## ## Model 1: aphids ~ temperature + humidity + I(temperature^2) + I(humidity^2)\r## Model 2: aphids ~ temperature + humidity + I(temperature^2)\r## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F)\r## 1 29 13581 ## 2 30 14386 -1 -805.11 1.7191 0.2001\r# Compare with baseline model\ranova(model_a, model_b3) ## this model is close to p=0,05 and probably has \r## Analysis of Variance Table\r## ## Model 1: aphids ~ temperature + humidity\r## Model 2: aphids ~ temperature + humidity + I(temperature^2)\r## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 31 16369 ## 2 30 14386 1 1982.4 4.1339 0.05096 .\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r# some predictive value\r# From model b2 (no interaction), we will now remove temperature to # look at the humidity^2 term\rmodel_b3t \u0026lt;- update(model_b2, .~.-I(temperature^2))\rsummary(model_b3t) # it appears that all factors explain something\r## ## Call:\r## lm(formula = aphids ~ temperature + humidity + I(humidity^2))\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -41.466 -10.864 -0.372 9.754 42.475 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 18.95111 50.44302 0.376 0.70979 ## temperature -1.15206 1.35435 -0.851 0.40171 ## humidity 3.24552 0.91764 3.537 0.00134 **\r## I(humidity^2) -0.02563 0.01080 -2.373 0.02426 * ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 21.43 on 30 degrees of freedom\r## Multiple R-squared: 0.6211, Adjusted R-squared: 0.5832 ## F-statistic: 16.39 on 3 and 30 DF, p-value: 1.711e-06\ranova(model_a, model_b3t)\r## Analysis of Variance Table\r## ## Model 1: aphids ~ temperature + humidity\r## Model 2: aphids ~ temperature + humidity + I(humidity^2)\r## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 31 16369 ## 2 30 13782 1 2586.5 5.63 0.02426 *\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\ranova(model_b2, model_b3t)\r## Analysis of Variance Table\r## ## Model 1: aphids ~ temperature + humidity + I(temperature^2) + I(humidity^2)\r## Model 2: aphids ~ temperature + humidity + I(humidity^2)\r## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F)\r## 1 29 13581 ## 2 30 13782 -1 -201.05 0.4293 0.5175\r# From model b3, remove temperature^2 (can do the same with model_b3t)\rmodel_b4 \u0026lt;- update(model_b3, .~.-I(temperature^2))\ranova(model_b4)\r## Analysis of Variance Table\r## ## Response: aphids\r## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## temperature 1 15194.8 15194.8 28.7765 7.554e-06 ***\r## humidity 1 4813.1 4813.1 9.1151 0.005038 ** ## Residuals 31 16368.9 528.0 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rsummary(model_b4)\r## ## Call:\r## lm(formula = aphids ~ temperature + humidity)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -35.393 -14.006 -3.198 10.335 49.265 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 35.8255 53.5388 0.669 0.50835 ## temperature -0.6765 1.4360 -0.471 0.64089 ## humidity 1.2811 0.4243 3.019 0.00504 **\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 22.98 on 31 degrees of freedom\r## Multiple R-squared: 0.55, Adjusted R-squared: 0.521 ## F-statistic: 18.95 on 2 and 31 DF, p-value: 4.212e-06\ranova(model_b3, model_b4) \r## Analysis of Variance Table\r## ## Model 1: aphids ~ temperature + humidity + I(temperature^2)\r## Model 2: aphids ~ temperature + humidity\r## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 30 14386 ## 2 31 16369 -1 -1982.4 4.1339 0.05096 .\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r# You can continue reducing the model, but we do know now that there is some predictive value with the variables we have \r\rStepwise methods\rNow, let’s use the function step() to automate the search process for the best model.\nWhat is required typically is the definition of the null model and the full model. With these defined, we can use “forward”, “backward”, or “both” direction searching.\n## Stepwise methods\r# Null model\rmodel_null \u0026lt;- lm(aphids~1, data=aphids_data)\rmodel_full \u0026lt;- model_b\r# Forward\rforward \u0026lt;- step(model_null, scope=list(lower=model_null, upper=model_full, direction=\u0026quot;forward\u0026quot;)) \r## Start: AIC=239.16\r## aphids ~ 1\r## ## Df Sum of Sq RSS AIC\r## + humidity 1 19891 16486 214.25\r## + I(temperature^2) 1 16098 20279 221.29\r## + I(humidity^2) 1 15560 20817 222.18\r## + temperature 1 15195 21182 222.77\r## \u0026lt;none\u0026gt; 36377 239.16\r## ## Step: AIC=214.25\r## aphids ~ humidity\r## ## Df Sum of Sq RSS AIC\r## + I(humidity^2) 1 2371.2 14115 210.97\r## \u0026lt;none\u0026gt; 16486 214.25\r## + I(temperature^2) 1 358.4 16128 215.51\r## + temperature 1 117.2 16369 216.01\r## - humidity 1 19890.7 36377 239.16\r## ## Step: AIC=210.97\r## aphids ~ humidity + I(humidity^2)\r## ## Df Sum of Sq RSS AIC\r## \u0026lt;none\u0026gt; 14115 210.97\r## + I(temperature^2) 1 400.9 13714 211.99\r## + temperature 1 332.4 13782 212.16\r## - I(humidity^2) 1 2371.2 16486 214.25\r## - humidity 1 6702.0 20817 222.18\r# Backwards\rback \u0026lt;- step(model_null, scope=list(lower=model_null, upper=model_full, direction=\u0026quot;backward\u0026quot;)) \r## Start: AIC=239.16\r## aphids ~ 1\r## ## Df Sum of Sq RSS AIC\r## + humidity 1 19891 16486 214.25\r## + I(temperature^2) 1 16098 20279 221.29\r## + I(humidity^2) 1 15560 20817 222.18\r## + temperature 1 15195 21182 222.77\r## \u0026lt;none\u0026gt; 36377 239.16\r## ## Step: AIC=214.25\r## aphids ~ humidity\r## ## Df Sum of Sq RSS AIC\r## + I(humidity^2) 1 2371.2 14115 210.97\r## \u0026lt;none\u0026gt; 16486 214.25\r## + I(temperature^2) 1 358.4 16128 215.51\r## + temperature 1 117.2 16369 216.01\r## - humidity 1 19890.7 36377 239.16\r## ## Step: AIC=210.97\r## aphids ~ humidity + I(humidity^2)\r## ## Df Sum of Sq RSS AIC\r## \u0026lt;none\u0026gt; 14115 210.97\r## + I(temperature^2) 1 400.9 13714 211.99\r## + temperature 1 332.4 13782 212.16\r## - I(humidity^2) 1 2371.2 16486 214.25\r## - humidity 1 6702.0 20817 222.18\r# Both directions back \u0026lt;- step(model_null, scope=list(lower=model_null, upper=model_full, direction=\u0026quot;both\u0026quot;))\r## Start: AIC=239.16\r## aphids ~ 1\r## ## Df Sum of Sq RSS AIC\r## + humidity 1 19891 16486 214.25\r## + I(temperature^2) 1 16098 20279 221.29\r## + I(humidity^2) 1 15560 20817 222.18\r## + temperature 1 15195 21182 222.77\r## \u0026lt;none\u0026gt; 36377 239.16\r## ## Step: AIC=214.25\r## aphids ~ humidity\r## ## Df Sum of Sq RSS AIC\r## + I(humidity^2) 1 2371.2 14115 210.97\r## \u0026lt;none\u0026gt; 16486 214.25\r## + I(temperature^2) 1 358.4 16128 215.51\r## + temperature 1 117.2 16369 216.01\r## - humidity 1 19890.7 36377 239.16\r## ## Step: AIC=210.97\r## aphids ~ humidity + I(humidity^2)\r## ## Df Sum of Sq RSS AIC\r## \u0026lt;none\u0026gt; 14115 210.97\r## + I(temperature^2) 1 400.9 13714 211.99\r## + temperature 1 332.4 13782 212.16\r## - I(humidity^2) 1 2371.2 16486 214.25\r## - humidity 1 6702.0 20817 222.18\r\rReturn to manual model\rFor the moment, let’s return to our manual model to take a look at suggest model from the stepwise procedure.\nmodel_b5\u0026lt;-with(aphids_data, lm(aphids~humidity+I(humidity^2)))\ranova(model_b5)\r## Analysis of Variance Table\r## ## Response: aphids\r## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## humidity 1 19890.7 19890.7 43.6854 2.194e-07 ***\r## I(humidity^2) 1 2371.2 2371.2 5.2079 0.0295 * ## Residuals 31 14114.8 455.3 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rsummary(model_b5)\r## ## Call:\r## lm(formula = aphids ~ humidity + I(humidity^2))\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -44.703 -13.018 -0.288 12.098 40.196 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -21.54288 16.60909 -1.297 0.204184 ## humidity 3.41812 0.89093 3.837 0.000574 ***\r## I(humidity^2) -0.02427 0.01063 -2.282 0.029504 * ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 21.34 on 31 degrees of freedom\r## Multiple R-squared: 0.612, Adjusted R-squared: 0.5869 ## F-statistic: 24.45 on 2 and 31 DF, p-value: 4.238e-07\r\rBest subsets\rIn the last section of this exercise, we will use a method based on best subsets regression. We will use the funtion regsubsets in the package leaps to do this exercise. This method looks at the full model and considers different combinations of models based on the number of best models we decide to examine. The result is not necessarily what is the best model but rather a series of models that explain something in our model. We would then need to go back, after model selection, and run the formal analysis to look at model fit, predictive value, biological relevance, etc..\n# regsubsets = package *leaps*\r# let\u0026#39;s start by looking at the best 3 models per level\rsubsets \u0026lt;- regsubsets(aphids~temperature+humidity+I(temperature^2)+\rI(humidity^2)+temperature:humidity, nbest=3, data=aphids_data)\rplot(subsets, scale=\u0026quot;adjr2\u0026quot;)\rplot(subsets, scale=\u0026quot;bic\u0026quot;)\rplot(subsets, scale=\u0026quot;Cp\u0026quot;)\r\rSummary\rHopefully you saw that this was not an easy exercise since there was not a clear model that best fit the response. In modeling we are integrating mathematical/statistical concepts with computational methodology, as well as keeping in mind the biology/pathology.\nWhat is the best method to model observaed relationships?\nFor this concluding part, I draw and adapt on ideas from Gelman and Hall (2007; Data Analysis Using Regression and Multilevel/Hierarchical Models):\nInclude all variables that for reasons known to the researcher may be important for predicting an outcome.\rYou do not always have to include all inputs as separate predictors. You can consider in some situations that several inputs could be averaged or summed to create a “total score” that then becomes the predictor variable (index value, etc.)\rFor predictive variables with large effects, an examination of the interactions may also be warranted (very common when we extend this to regression trees and other methods).\rStrategy for decisions focused on excluding a variable based on the expected sign and statistical significance:\r\r\rIf the predictor is not statistically significant and has the expected sign (+ or -), in general it is fine to keep the predictor. This means that while the predictor is not helping predictions dramatically, it is also not hurting them.\rIf the predictor is not statistically significant and does not have the expected sign, consider removing this from the model (i.e., the coefficiente is set to 0).\rIf the predictor is statistically significant but does not have the expected sign, this is somewhat more complicated since the challenge is in terms of interpretation. Consider trying to gather additional data on lurking variables and include those in the analysis.\rIf the predictor is statistically significant and has the expected sign, definitely you should keep this in the model!\r\r\r","date":1564711689,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564711689,"objectID":"42c55f36c72a1b1529d01720b42fb3a3","permalink":"/post/regression_modeling/","publishdate":"2019-08-01T21:08:09-05:00","relpermalink":"/post/regression_modeling/","section":"post","summary":"Background\rWhen building a model, there are different methods we can take to construct it, ranging from manual to automated. There are strengths and weaknesses in using the different methods, but they provide a good background for those interested in taking their models to a higher level (machine level, etc.), since in those situations we are often interested to look for interactions that cannot easily be found with basic approaches.","tags":["Regression"],"title":"Modeling methods for regression","type":"post"},{"authors":["Paul Esker and Felipe Dalla Lana"],"categories":["R"],"content":"\rBackground\rGiven the background and tools presented in linear regression, we will not extend the modeling approach to include additional variables, as well as relationships that are more complicated. This exercise provides the jumping off point for more automated modeling approaches, which will we see in the subsequent example(s).\nOur assumption in this exercise is that multiple factors have explanatory value to explain the response variable of interest.\nWhat does a model of this type look like? Some examples include:\nAdditive.\r\r\\(Y = \\beta_0 + \\beta_1{X_1} + \\beta_2{X_2} + \\epsilon\\)\nWith interaction between the two terms.\r\r\\(Y = \\beta_0 + \\beta_1{X_1} + \\beta_2{X_2} + \\beta_3{X_1}{X_2} + \\epsilon\\)\nNote: It is important to note that in modeling, when we add new explanatory variables that have merit (i.e., the sign makes sense in terms of the biological relation), the model will improve. This is not necessarily the same as being “biologically relevant”. We should always consider the variable in the context of the question of interest.\nlibrary(tidyverse)\rlibrary(Hmisc)\rlibrary(corrplot)\rlibrary(readr)\rlibrary(HH)\rlibrary(car)\rlibrary(scatterplot3d)\rlibrary(olsrr)\r\rData and exploratory analysis\rOur database comes from counts of the number of aphids in different lots, as well as measures of average temperature and relative humidity. We assume that there is a relationship between those two latter factors with the observed number of aphids, which means from a predictive value, we hope that by just measuring T and RH, we can estimate the number of expected aphids.\nWhere do we start? The main question is to determine if there is (are) a relationship between T and RH with the counts. We are also interested in trying to determine if there may be a complext relationship (i.e., that the predictive values have some degree of interpretable interaction).\nlot \u0026lt;- c(1:34)\raphids \u0026lt;- c(61, 77, 87, 93, 98, 100, 104, 118, 102, 74, 63, 43, 27, 19, 14, 23, 30, 25, 67, 40, 6, 21, 18, 23, 42, 56, 60, 59, 82, 89, 77, 102, 108, 97)\rtemperature \u0026lt;- c(21, 24.8, 28.3, 26, 27.5, 27.1, 26.8, 29, 28.3, 34, 30.5, 28.3, 30.8, 31, 33.6, 31.8, 31.3, 33.5, 33, 34.5, 34.3, 34.3, 33, 26.5, 32, 27.3, 27.8, 25.8, 25, 18.5, 26, 19, 18, 16.3)\rhumidity \u0026lt;- c(57,48, 41.5, 56, 58, 31, 36.5, 41, 40, 25, 34, 13, 37, 19, 20, 17, 21, 18.5, 24.5, 16, 6, 26, 21, 26, 28, 24.5, 39, 29, 41, 53.5, 51, 48, 70, 79.5)\raphids_data \u0026lt;- data.frame(lot, aphids, temperature, humidity)\r# Quick exploratory analysis\rsummary(aphids_data)\r## lot aphids temperature humidity ## Min. : 1.00 Min. : 6.00 Min. :16.30 Min. : 6.00 ## 1st Qu.: 9.25 1st Qu.: 27.75 1st Qu.:26.00 1st Qu.:21.88 ## Median :17.50 Median : 62.00 Median :28.30 Median :32.50 ## Mean :17.50 Mean : 61.91 Mean :28.09 Mean :35.19 ## 3rd Qu.:25.75 3rd Qu.: 92.00 3rd Qu.:31.95 3rd Qu.:46.38 ## Max. :34.00 Max. :118.00 Max. :34.50 Max. :79.50\rcor(aphids_data[,2:4])\r## aphids temperature humidity\r## aphids 1.0000000 -0.6463022 0.7394570\r## temperature -0.6463022 1.0000000 -0.8313696\r## humidity 0.7394570 -0.8313696 1.0000000\rplot(aphids_data[,2:4]) # Graphical matrix\rpairs(aphids_data[,2:4]) # Gives us the same thing\r\rLinear regression\r# Factor = temperature (X)\rmodel1\u0026lt;-with(aphids_data, lm(aphids~temperature))\ranova(model1)\r## Analysis of Variance Table\r## ## Response: aphids\r## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## temperature 1 15195 15194.8 22.955 3.643e-05 ***\r## Residuals 32 21182 661.9 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rsummary(model1)\r## ## Call:\r## lm(formula = aphids ~ temperature)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -45.698 -18.111 -3.143 19.477 60.004 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 182.1386 25.4785 7.149 4.10e-08 ***\r## temperature -4.2808 0.8935 -4.791 3.64e-05 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 25.73 on 32 degrees of freedom\r## Multiple R-squared: 0.4177, Adjusted R-squared: 0.3995 ## F-statistic: 22.96 on 1 and 32 DF, p-value: 3.643e-05\rplot(model1)\r# Assumptions = values, model1\rrstudent(model1)\r## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## -1.28585895 0.04005818 1.02696012 0.87344711 1.34166917 1.35439948 1.47093718 2.56708773 1.66182645 1.54106791 0.44669460 -0.70426322 -0.92092001 -1.21610745 -0.97682293 -0.91330799 -0.71519631 -0.54584549 1.04821427 0.22134400 -1.19286546 -0.57242992 -0.91388994 -1.87539948 -0.12367881 -0.36099265 -0.12169500 -0.49651581 0.26909704 -0.57840180 0.24013268 0.04903167 0.12114783 -0.66038629\rdfbetas(model1)\r## (Intercept) temperature\r## 1 -0.366682839 0.331661207\r## 2 0.005815635 -0.004670407\r## 3 0.023305013 0.007772618\r## 4 0.089808162 -0.064378045\r## 5 0.067723562 -0.027686588\r## 6 0.087212625 -0.047068694\r## 7 0.110092705 -0.066711426\r## 8 -0.004133934 0.082814320\r## 9 0.037712163 0.012577648\r## 10 -0.276055170 0.328520764\r## 11 -0.024068323 0.038160258\r## 12 -0.015981987 -0.005330265\r## 13 0.059303647 -0.088531881\r## 14 0.086856812 -0.125611236\r## 15 0.160634492 -0.193579923\r## 16 0.091034920 -0.120629792\r## 17 0.058637009 -0.081569976\r## 18 0.087768241 -0.106135446\r## 19 -0.149512336 0.184383492\r## 20 -0.043753984 0.051380499\r## 21 0.226920851 -0.267823576\r## 22 0.108894329 -0.128522648\r## 23 0.130352947 -0.160755509\r## 24 -0.160003296 0.104963953\r## 25 0.013206774 -0.017231640\r## 26 -0.020732461 0.009996651\r## 27 -0.004874275 0.001223897\r## 28 -0.054538615 0.040127888\r## 29 0.037156513 -0.029440595\r## 30 -0.223031160 0.207642177\r## 31 0.024690533 -0.017699151\r## 32 0.017885539 -0.016575675\r## 33 0.049290028 -0.046078814\r## 34 -0.318930693 0.301601424\rdffits(model1)\r## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## -0.404272806 0.008432063 0.178944815 0.165495030 0.235239322 0.240562702 0.264859612 0.454709984 0.289568427 0.427975171 0.086872782 -0.122715819 -0.183780341 -0.247127470 -0.259852603 -0.200671906 -0.149517430 -0.143648162 0.261386769 0.064842854 -0.342084958 -0.164159053 -0.227891133 -0.343410519 -0.027739070 -0.063654708 -0.021220776 -0.095548871 0.055563960 -0.233580039 0.045498765 0.018866121 0.051306429 -0.327009697\rcovratio(model1)\r## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## 1.0553084 1.1126545 1.0268519 1.0514226 0.9810703 0.9797855 0.9612415 0.7474350 0.9256398 0.9902076 1.0917586 1.0636026 1.0497679 1.0108127 1.0738384 1.0592292 1.0763152 1.1177641 1.0556567 1.1533544 1.0541908 1.1291908 1.0732083 0.8882885 1.1180537 1.0895089 1.0969090 1.0876492 1.1058148 1.2130097 1.0997154 1.2231239 1.2554800 1.2902753\rcooks.distance(model1)\r## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## 8.008297e-02 3.669472e-05 1.598333e-02 1.379652e-02 2.699386e-02 2.819990e-02 3.384457e-02 8.800702e-02 3.973731e-02 8.780865e-02 3.870253e-03 7.650078e-03 1.696816e-02 3.008573e-02 3.381010e-02 2.023952e-02 1.135101e-02 1.054883e-02 3.405642e-02 2.166690e-03 5.774783e-02 1.376327e-02 2.610161e-02 5.466541e-02 3.969427e-04 2.082560e-03 2.323129e-04 4.674868e-03 1.589759e-03 2.785916e-02 1.066474e-03 1.836918e-04 1.357989e-03 5.442676e-02\r# Factor = humedad (X)\rmodel2\u0026lt;-with(aphids_data, lm(aphids~humidity))\ranova(model2)\r## Analysis of Variance Table\r## ## Response: aphids\r## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## humidity 1 19891 19890.7 38.608 5.857e-07 ***\r## Residuals 32 16486 515.2 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rsummary(model2)\r## ## Call:\r## lm(formula = aphids ~ humidity)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -37.53 -13.44 -1.43 12.82 47.68 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 10.9787 9.0744 1.210 0.235 ## humidity 1.4473 0.2329 6.214 5.86e-07 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 22.7 on 32 degrees of freedom\r## Multiple R-squared: 0.5468, Adjusted R-squared: 0.5326 ## F-statistic: 38.61 on 1 and 32 DF, p-value: 5.857e-07\rplot(model2)\r# Assumptions = values, model2\rrstudent(model2)\r## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## -1.52166116 -0.15329366 0.70958337 0.04378704 0.13944816 2.07616739 1.86604477 2.27067980 1.51293060 1.21600986 0.12382267 0.60092052 -1.73010695 -0.88059890 -1.18139920 -0.56699356 -0.50823171 -0.57307681 0.92313524 0.26372262 -0.63535605 -1.25128763 -1.05882161 -1.15657354 -0.42068829 0.42473282 -0.32760978 0.26710592 0.51730586 0.02642984 -0.34840648 0.97153856 -0.20281479 -1.49172395\rdfbetas(model2)\r## (Intercept) humidity\r## 1 0.203962802 -0.354960067\r## 2 0.007091915 -0.020637509\r## 3 0.010888321 0.046732070\r## 4 -0.005432893 0.009722237\r## 5 -0.020090271 0.034108001\r## 6 0.237139151 -0.090726882\r## 7 0.116375070 0.025442918\r## 8 0.045533868 0.137646115\r## 9 0.044574968 0.075879906\r## 10 0.208590411 -0.129821254\r## 11 0.010635007 -0.001536502\r## 12 0.175091541 -0.142772652\r## 13 -0.099765368 -0.032603908\r## 14 -0.202822717 0.150676741\r## 15 -0.260370374 0.189329385\r## 16 -0.141963444 0.109421496\r## 17 -0.106991971 0.075962840\r## 18 -0.134852336 0.101178572\r## 19 0.162812727 -0.103448492\r## 20 0.068702630 -0.053805692\r## 21 -0.232992130 0.202795860\r## 22 -0.202585666 0.120351428\r## 23 -0.222901106 0.158256745\r## 24 -0.187251287 0.111241631\r## 25 -0.060049112 0.031601350\r## 26 0.074909835 -0.047596460\r## 27 -0.012732795 -0.013008080\r## 28 0.035580371 -0.017261761\r## 29 0.010373518 0.031358513\r## 30 -0.002627836 0.005134802\r## 31 0.026166587 -0.058167303\r## 32 -0.044946864 0.130795599\r## 33 0.055027998 -0.078907786\r## 34 0.575503750 -0.776106220\rdffits(model2)\r## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## -0.447191192 -0.033924951 0.132317421 0.012469416 0.042283274 0.372962648 0.325861688 0.419240517 0.274398674 0.249344653 0.021611110 0.178729735 -0.302985776 -0.216541182 -0.281470975 -0.148585846 -0.117356163 -0.143175941 0.191962176 0.071346658 -0.233677243 -0.249738808 -0.244493286 -0.230835253 -0.079949336 0.088321443 -0.058538076 0.049688881 0.095511299 0.006952189 -0.084642540 0.215008230 -0.087530558 -0.829472811\rcovratio(model2)\r## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## 1.0022713 1.1160516 1.0676446 1.1518269 1.1620673 0.8477864 0.8874781 0.8100232 0.9544553 1.0115565 1.0969300 1.1332631 0.9133416 1.0755087 1.0311057 1.1154780 1.1038994 1.1084567 1.0529470 1.1384310 1.1787935 1.0040208 1.0453923 1.0182323 1.0915424 1.0988072 1.0920026 1.0973744 1.0831002 1.1392331 1.1196610 1.0526653 1.2606798 1.2144142\rcooks.distance(model2)\r## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## 9.604190e-02 5.935641e-04 8.891911e-03 8.024605e-05 9.221958e-04 6.302998e-02 4.927114e-02 7.777970e-02 3.618960e-02 3.062822e-02 2.409338e-04 1.629755e-02 4.320873e-02 2.361072e-02 3.912909e-02 1.127801e-02 7.049632e-03 1.046940e-02 1.851025e-02 2.621394e-03 2.782097e-02 3.064301e-02 2.977580e-02 2.636426e-02 3.280316e-03 4.002862e-03 1.762520e-03 1.271389e-03 4.668043e-03 2.494547e-05 3.683311e-03 2.315487e-02 3.949133e-03 3.313265e-01\r\rAdditive multiple regression\rModel form:\n\\[aphids = intercept + temperature + humidity + error\\]\nmodel3\u0026lt;-with(aphids_data, lm(aphids~temperature+humidity))\ranova(model3)\r## Analysis of Variance Table\r## ## Response: aphids\r## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## temperature 1 15194.8 15194.8 28.7765 7.554e-06 ***\r## humidity 1 4813.1 4813.1 9.1151 0.005038 ** ## Residuals 31 16368.9 528.0 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rsummary(model3)\r## ## Call:\r## lm(formula = aphids ~ temperature + humidity)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -35.393 -14.006 -3.198 10.335 49.265 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 35.8255 53.5388 0.669 0.50835 ## temperature -0.6765 1.4360 -0.471 0.64089 ## humidity 1.2811 0.4243 3.019 0.00504 **\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 22.98 on 31 degrees of freedom\r## Multiple R-squared: 0.55, Adjusted R-squared: 0.521 ## F-statistic: 18.95 on 2 and 31 DF, p-value: 4.212e-06\rplot(model3)\rvif(lm(aphids~temperature+humidity, data=aphids_data))\r## temperature humidity ## 3.238084 3.238084\r# Assumptions = values, model3\rrstudent(model3)\r## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## -1.5718383 -0.1554587 0.7588243 0.1370895 0.3068861 1.9975726 1.8135772 2.3619288 1.5465227 1.3436594 0.1864002 0.4608253 -1.6388797 -0.9045838 -1.1176402 -0.5834426 -0.5100256 -0.5278939 0.9931907 0.3134810 -0.6587793 -1.1492651 -1.0051359 -1.3057363 -0.3548909 0.3255590 -0.3044695 0.1559741 0.4639182 -0.1337120 -0.2920664 0.8408516 -0.2500954 -1.5096341\rdfbetas(model3)\r## (Intercept) temperature humidity\r## 1 -0.1389605773 0.177983609 -0.057094981\r## 2 -0.0001234515 0.001378033 -0.010485457\r## 3 -0.0823800284 0.085661206 0.099164620\r## 4 -0.0300681549 0.027499212 0.040114310\r## 5 -0.1128940158 0.106442807 0.132644719\r## 6 0.2933256395 -0.257672477 -0.263133078\r## 7 0.1288811603 -0.111084942 -0.078585304\r## 8 -0.3419564458 0.355447009 0.375970378\r## 9 -0.1277781057 0.137669370 0.157728821\r## 10 -0.2545839017 0.299547009 0.167359803\r## 11 -0.0221748178 0.025322754 0.019755369\r## 12 0.1894356505 -0.167405618 -0.203910662\r## 13 0.3137416001 -0.335266154 -0.296248797\r## 14 -0.0969427761 0.062028386 0.137785229\r## 15 0.0843787510 -0.128835461 -0.006914942\r## 16 -0.0531420869 0.028468182 0.086313818\r## 17 -0.0271836261 0.008889032 0.049759567\r## 18 0.0227872585 -0.044844187 0.014698315\r## 19 -0.1140684195 0.146626191 0.059379069\r## 20 -0.0200956838 0.034709112 -0.006903602\r## 21 -0.0829693398 0.042055524 0.152053987\r## 22 0.2620078051 -0.299442541 -0.185468011\r## 23 0.0546306003 -0.092463785 0.006968177\r## 24 -0.3623565160 0.329834636 0.346198714\r## 25 0.0394488839 -0.048949485 -0.025740050\r## 26 0.0816581713 -0.072640776 -0.081164117\r## 27 0.0103635254 -0.012582384 -0.017184593\r## 28 0.0419895172 -0.038892078 -0.038106780\r## 29 0.0500352973 -0.049159176 -0.025153876\r## 30 -0.0434351938 0.046540937 0.023406939\r## 31 0.0372940235 -0.034009119 -0.055554804\r## 32 0.3331825303 -0.345523841 -0.219245482\r## 33 -0.0141526880 0.026249474 -0.032547183\r## 34 0.0042654848 0.097322196 -0.356471323\rdffits(model3)\r## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## -0.49779546 -0.03443304 0.16617790 0.04839027 0.14501981 0.44419213 0.33617659 0.56641172 0.31345118 0.41159699 0.04146285 0.22201280 -0.44522486 -0.23142890 -0.29739864 -0.15570300 -0.11812321 -0.13975284 0.25511469 0.09211577 -0.24640080 -0.38190475 -0.25074744 -0.42548793 -0.08385349 0.10043884 -0.05588462 0.04905946 0.09917484 -0.05960708 -0.07911715 0.39982748 -0.11165807 -0.84678558\rcovratio(model3)\r## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## 0.9574605 1.1547079 1.0921820 1.2385180 1.3371269 0.7961237 0.8353191 0.6995172 0.9125707 1.0128230 1.1539505 1.3310018 0.9160657 1.0844138 1.0454007 1.1426135 1.1328307 1.1483997 1.0673803 1.1869404 1.2046850 1.0766525 1.0611755 1.0340267 1.1504192 1.1956712 1.1300346 1.2095848 1.1293150 1.3202770 1.1742902 1.2615329 1.3150607 1.1644730\rcooks.distance(model3)\r## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## 0.0788589481 0.0004080564 0.0093327348 0.0008060524 0.0072212533 0.0599828648 0.0350811487 0.0931782892 0.0313433979 0.0550406641 0.0005914727 0.0168582238 0.0626669338 0.0179583880 0.0292469522 0.0082568244 0.0047647510 0.0066653796 0.0217040047 0.0029131770 0.0206141656 0.0481191084 0.0209511332 0.0590048734 0.0024118042 0.0034625093 0.0010724176 0.0008283478 0.0033637037 0.0012230834 0.0021499446 0.0537957394 0.0042854350 0.2295447363\r\rVisualizing more complicated relationships\rSo far, we cannot say with certainty that the additive model is the best fitting model. Before we commit to another analysis, it is important to take a step back and think about the visualization of the data to be better informed about what has occurred. Another reason for doing this is to be able to better interpret the observed results about the model assumptions (i.e., influential observations, some unhidden spatial structure in the data collection process).\n# Start with temperature, let\u0026#39;s add to the graph infomration about the lots\rtemp \u0026lt;- ggplot(aphids_data, aes(x=temperature, y=aphids, label=lot))\rtemp + geom_point() + geom_text(hjust=0.5, nudge_y=3) #Have a look a few of the observations like 30, 32, 33, 34 and also 8 (maybe 9)\r# Now let\u0026#39;s consider RH and do the same thing\rtemp2 \u0026lt;- ggplot(aphids_data, aes(x=humidity, y=aphids, label=lot))\rtemp2 + geom_point() + geom_text(hjust=0.5, nudge_y=3) #Maybe a bit different grouping\u0026quot; 6-9, 33 and 34\r# In 3-dimensiones? This example comes from the package *scatterplot3d*\rwith(aphids_data, scatterplot3d(temperature, humidity, aphids, angle=75)) \r\rMultiple regression with interactions\rGiven that individually, we see different relationships between the number of aphids with temperature or relative humidity, we might want to consider if there is an interaction between those two factors that helps to explain the overall relationship.\nmodel4 \u0026lt;- with(aphids_data, lm(aphids ~ temperature + humidity + temperature:humidity))\ranova(model4)\r## Analysis of Variance Table\r## ## Response: aphids\r## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## temperature 1 15194.8 15194.8 33.506 2.522e-06 ***\r## humidity 1 4813.1 4813.1 10.613 0.002789 ** ## temperature:humidity 1 2764.0 2764.0 6.095 0.019474 * ## Residuals 30 13604.8 453.5 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rsummary(model4) \r## ## Call:\r## lm(formula = aphids ~ temperature + humidity + temperature:humidity)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -41.13 -12.87 -2.02 10.25 41.75 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 150.70989 68.02395 2.216 0.0345 *\r## temperature -4.72276 2.11121 -2.237 0.0329 *\r## humidity -1.29670 1.11576 -1.162 0.2543 ## temperature:humidity 0.09728 0.03940 2.469 0.0195 *\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 21.3 on 30 degrees of freedom\r## Multiple R-squared: 0.626, Adjusted R-squared: 0.5886 ## F-statistic: 16.74 on 3 and 30 DF, p-value: 1.414e-06\rplot(model4)\r# Assumptions = values, model4\rrstudent(model4)\r## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## -1.67738460 -0.48588149 0.45590218 -0.19523576 -0.14531052 1.79972032 1.58449094 2.15886689 1.30727332 1.70840183 -0.02181518 0.36068272 -2.12994645 -0.86808331 -0.85354897 -0.38866846 -0.45693592 -0.18387477 1.23775270 0.97153371 0.27052672 -1.03205411 -0.82508336 -1.86536030 -0.40145315 0.04498291 -0.68446574 -0.24802524 0.13423194 -0.06323885 -0.67115336 0.75218833 0.56505021 0.02067808\rdfbetas(model4)\r## (Intercept) temperature humidity temperature:humidity\r## 1 -0.0949334782 0.104709362 -0.039584196 0.019350004\r## 2 -0.0434083991 0.051676363 0.047346765 -0.063039334\r## 3 0.0104511179 -0.020483667 -0.043004153 0.068644149\r## 4 0.0125973436 -0.003428996 0.005585795 -0.027700059\r## 5 0.0200060874 -0.009982149 0.004443134 -0.028864368\r## 6 0.3651795277 -0.341307478 -0.317703892 0.249350618\r## 7 0.2422524883 -0.242541747 -0.242297450 0.232776103\r## 8 -0.0109075193 -0.042140260 -0.177929199 0.320976539\r## 9 0.0498830523 -0.072826504 -0.129655838 0.189283832\r## 10 -0.3408072725 0.358785369 0.217261311 -0.151702669\r## 11 0.0005263790 -0.000316099 0.001062068 -0.002009342\r## 12 0.1221549093 -0.098424293 -0.075269257 0.020228157\r## 13 0.1333989598 -0.088122021 0.090421220 -0.242562866\r## 14 -0.0418864697 0.008011790 0.011031748 0.038058296\r## 15 0.1317295973 -0.158260224 -0.117120422 0.123141864\r## 16 0.0004605123 -0.017956896 -0.015762200 0.038604856\r## 17 -0.0058300098 -0.008533870 -0.000619253 0.017463648\r## 18 0.0260840050 -0.032918740 -0.025833607 0.029557943\r## 19 -0.1563373872 0.174924757 0.097882894 -0.076671310\r## 20 -0.2133463221 0.258866254 0.220017614 -0.243411245\r## 21 -0.0520591385 0.077908687 0.084254567 -0.115605524\r## 22 0.2314600667 -0.237322197 -0.139947666 0.086596590\r## 23 0.0925507109 -0.115811898 -0.079585871 0.087209322\r## 24 -0.5799971247 0.525151313 0.447129697 -0.289295996\r## 25 0.0304683047 -0.032541539 -0.007414919 -0.003043043\r## 26 0.0122044840 -0.010813140 -0.009331422 0.005713728\r## 27 -0.0499259797 0.058126678 0.078027579 -0.098076089\r## 28 -0.0786659661 0.072746679 0.061679949 -0.042751118\r## 29 0.0246830038 -0.024958336 -0.021748326 0.020466850\r## 30 -0.0135483304 0.012244012 0.001928902 0.002110430\r## 31 -0.0027225130 0.024986567 0.044656549 -0.096290927\r## 32 0.2502942582 -0.232060720 -0.113674644 0.047457190\r## 33 -0.1102356148 0.113536109 0.212046646 -0.197252978\r## 34 -0.0122551182 0.012733935 0.018960381 -0.017832210\rdffits(model4)\r## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## -0.531609206 -0.125502391 0.122090070 -0.074914358 -0.075725395 0.474770847 0.377244053 0.613987196 0.327877853 0.546849311 -0.005271111 0.175211526 -0.630865880 -0.225538194 -0.260429456 -0.111153445 -0.107334987 -0.057484681 0.327640435 0.381923652 0.159601123 -0.354887689 -0.224599512 -0.679748019 -0.094906727 0.015111134 -0.160394451 -0.089969303 0.035517257 -0.028285597 -0.207379175 0.361507505 0.332123809 0.023506774\rcovratio(model4)\r## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## 0.8701623 1.1826549 1.1927973 1.3069641 1.4520128 0.8020052 0.8681666 0.6819798 0.9680977 0.8603410 1.2120133 1.3903653 0.6965068 1.1033099 1.1335691 1.2134151 1.1742397 1.2513168 0.9974108 1.1632187 1.5283511 1.1085842 1.1210624 0.8245063 1.1827248 1.2741137 1.1331044 1.2849849 1.2223690 1.3735892 1.1795590 1.3049070 1.4748564 2.6250656\rcooks.distance(model4)\r## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## 6.662438e-02 4.040602e-03 3.827564e-03 1.449516e-03 1.481939e-03 5.243821e-02 3.387265e-02 8.399563e-02 2.625550e-02 7.026714e-02 7.185558e-06 7.903960e-03 8.900520e-02 1.282220e-02 1.711070e-02 3.178723e-03 2.958219e-03 8.536139e-04 2.636942e-02 3.653477e-02 6.571137e-03 3.141810e-02 1.274688e-02 1.066957e-01 2.316596e-03 5.905097e-05 6.547598e-03 2.088968e-03 3.260411e-04 2.068874e-04 1.095216e-02 3.315175e-02 2.821681e-02 1.429035e-04\r# Graphically from olsrr package\rols_plot_resid_stud(model4)\rols_plot_dfbetas(model4)\rols_plot_dffits(model4)\rols_plot_cooksd_chart(model4)\r# Compare the different models\ranova(model1, model3) # model 3 better\r## Analysis of Variance Table\r## ## Model 1: aphids ~ temperature\r## Model 2: aphids ~ temperature + humidity\r## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 32 21182 ## 2 31 16369 1 4813.1 9.1151 0.005038 **\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\ranova(model2, model3) # model 2 mejor (only RH)\r## Analysis of Variance Table\r## ## Model 1: aphids ~ humidity\r## Model 2: aphids ~ temperature + humidity\r## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F)\r## 1 32 16486 ## 2 31 16369 1 117.18 0.2219 0.6409\ranova(model2, model4) # the interaction improved the model?\r## Analysis of Variance Table\r## ## Model 1: aphids ~ humidity\r## Model 2: aphids ~ temperature + humidity + temperature:humidity\r## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 32 16486 ## 2 30 13605 2 2881.2 3.1767 0.05606 .\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\ranova(model3, model4) # the interaction improved the model\r## Analysis of Variance Table\r## ## Model 1: aphids ~ temperature + humidity\r## Model 2: aphids ~ temperature + humidity + temperature:humidity\r## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 31 16369 ## 2 30 13605 1 2764 6.095 0.01947 *\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r# Remember that once we have a model selected, we should examine the assumptions in greater detail\r\rPredictions\rTo close our discussion, let’s again look at the function predict using model 4.\n# Let\u0026#39;s start by considering the average values for temperature and relative humidity\rmean(temperature)\r## [1] 28.08529\rmean(humidity)\r## [1] 35.19118\robservation \u0026lt;- data.frame(temperature=mean(temperature), humidity=mean(humidity))\rpredict(object=model4, newdata=observation, interval=\u0026quot;confidence\u0026quot;)\r## fit lwr upr\r## 1 68.58645 59.30643 77.86646\rpredict(object=model4, newdata=observation, interval=\u0026quot;predict\u0026quot;)\r## fit lwr upr\r## 1 68.58645 24.11635 113.0565\r# Looking at all observations in the database\rintervals\u0026lt;-predict(model4, interval=\u0026quot;confidence\u0026quot;)\rintervals\r## fit lwr upr\r## 1 94.0665692 80.927158 107.20598\r## 2 87.1482874 76.271599 98.02498\r## 3 77.4955105 66.245093 88.74593\r## 4 96.9454425 81.365016 112.52587\r## 5 100.7900752 80.691122 120.88903\r## 6 64.2519748 53.158443 75.34551\r## 7 71.9715847 61.898562 82.04461\r## 8 76.2533767 64.356209 88.15054\r## 9 75.3109441 64.730640 85.89125\r## 10 40.4082060 27.149656 53.66676\r## 11 63.4592842 53.244672 73.67390\r## 12 35.9887485 16.985340 54.99216\r## 13 68.1334763 55.782306 80.48465\r## 14 36.9661205 26.029726 47.90252\r## 15 31.4646380 18.772561 44.15671\r## 16 31.0728691 19.114485 43.03125\r## 17 39.6002445 29.654831 49.54566\r## 18 28.7989865 15.821797 41.77618\r## 19 41.7421198 30.613084 52.87116\r## 20 20.7271297 4.815514 36.63875\r## 21 0.9596999 -21.139235 23.05864\r## 22 41.7610595 27.618756 55.90336\r## 23 35.0445138 23.621299 46.46773\r## 24 58.8698369 43.979310 73.76036\r## 25 50.4385954 40.432771 60.44442\r## 26 55.0764466 41.227039 68.92585\r## 27 74.3189517 64.396268 84.24164\r## 28 64.0447598 49.214277 78.87524\r## 29 79.1902050 68.065480 90.31493\r## 30 90.2502594 72.492852 108.00767\r## 31 90.7822943 77.942971 103.62162\r## 32 87.4570502 68.617765 106.29634\r## 33 97.5065531 75.468470 119.54464\r## 34 96.7041862 64.049441 129.35893\rpredictions\u0026lt;-predict(model4, interval=\u0026quot;predict\u0026quot;)\r## Warning in predict.lm(model4, interval = \u0026quot;predict\u0026quot;): predictions on current data refer to _future_ responses\rpredictions\r## fit lwr upr\r## 1 94.0665692 48.634034 139.49910\r## 2 87.1482874 42.317791 131.97878\r## 3 77.4955105 32.572877 122.41814\r## 4 96.9454425 50.747815 143.14307\r## 5 100.7900752 52.879335 148.70082\r## 6 64.2519748 19.368375 109.13557\r## 7 71.9715847 27.329263 116.61391\r## 8 76.2533767 31.164423 121.34233\r## 9 75.3109441 30.551432 120.07046\r## 10 40.4082060 -5.058928 85.87534\r## 11 63.4592842 18.784802 108.13377\r## 12 35.9887485 -11.472822 83.45032\r## 13 68.1334763 22.922609 113.34434\r## 14 36.9661205 -7.878900 81.81114\r## 15 31.4646380 -13.840548 76.76982\r## 16 31.0728691 -14.032275 76.17801\r## 17 39.6002445 -5.013457 84.21395\r## 18 28.7989865 -16.586898 74.18487\r## 19 41.7421198 -3.150269 86.63451\r## 20 20.7271297 -25.583243 67.03750\r## 21 0.9596999 -47.823843 49.74324\r## 22 41.7610595 -3.971597 87.49372\r## 23 35.0445138 -9.921706 80.01073\r## 24 58.8698369 12.900294 104.83938\r## 25 50.4385954 5.811388 95.06580\r## 26 55.0764466 9.433515 100.71938\r## 27 74.3189517 29.710312 118.92759\r## 28 64.0447598 18.094631 109.99489\r## 29 79.1902050 34.298885 124.08152\r## 30 90.2502594 43.273705 137.22681\r## 31 90.7822943 45.435637 136.12895\r## 32 87.4570502 40.060956 134.85314\r## 33 97.5065531 48.750546 146.26256\r## 34 96.7041862 42.318494 151.08988\r\rSummary\rThe objective in this exercise was to introduce the concept of using multiple regression to build a model. This provides a base also as you move forward in your modeling work to think about things like hidden interactions, which is often very common in complex datasets and can often drive aspects of things like machine learning.\n\r","date":1564711688,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564711688,"objectID":"cc8c1d9ab5db6426c09d373afee6fc47","permalink":"/post/multiple_regression/","publishdate":"2019-08-01T21:08:08-05:00","relpermalink":"/post/multiple_regression/","section":"post","summary":"Background\rGiven the background and tools presented in linear regression, we will not extend the modeling approach to include additional variables, as well as relationships that are more complicated. This exercise provides the jumping off point for more automated modeling approaches, which will we see in the subsequent example(s).\nOur assumption in this exercise is that multiple factors have explanatory value to explain the response variable of interest.\nWhat does a model of this type look like?","tags":["Regression"],"title":"Multiple regression","type":"post"},{"authors":["Paul Esker and Felipe Dalla Lana"],"categories":["R"],"content":"\rBackground\rIn many studies, for example if one looks the relationship between nitrogen and yield for many cereal crops, the relationship is not linear, rather there is often a plateau where after a specific amount, the response decreases. A simpler linear-type model will explain some of the variability, but not very well. In these situations we can consider a polynomial form to the model.\nWe can define this relationship in general terms as the the relation betweeen the independent variable, \\(x\\), and the expected response, \\(E(y|x)\\).\nNote: Very important with this type of analysis is to understand the software that you are using since often we focus on use \\(X\\) and \\(X^2\\), which depending on how those variables are defined, leads to high collinearity. This example illustrates that concept and provides methods to overcome the issue.\nlibrary(tidyverse)\rlibrary(Hmisc)\rlibrary(corrplot)\rlibrary(readr)\rlibrary(HH)\rlibrary(car)\r\rData\rFor this example, we have the following situation:\n\rDensity = Seeding density (number of plants per \\(m^2\\))\rYield = quantity of biomass\r\rdensity \u0026lt;- rep(c(10,20,30,40,50), each=3)\ryield \u0026lt;- c(12.2, 11.4, 12.4, 16, 15.5, 16.5, 18.6, 20.2, 18.2, 17.6, 19.3, 17.1, 18, 16.4, 16.6)\rdensities \u0026lt;- data.frame(density, yield)\r\rLinear regression\rTo start, we will build a simple linear regression models and examine the overall model fit, including model assumptions.\nmodel1 \u0026lt;- lm(yield~density)\ranova(model1)\r## Analysis of Variance Table\r## ## Response: yield\r## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## density 1 43.20 43.200 10.825 0.005858 **\r## Residuals 13 51.88 3.991 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rsummary(model1)\r## ## Call:\r## lm(formula = yield ~ density)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -2.6 -1.7 0.0 1.5 3.8 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 12.80000 1.20966 10.58 9.3e-08 ***\r## density 0.12000 0.03647 3.29 0.00586 ** ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 1.998 on 13 degrees of freedom\r## Multiple R-squared: 0.4544, Adjusted R-squared: 0.4124 ## F-statistic: 10.82 on 1 and 13 DF, p-value: 0.005858\rplot(model1) # You hopefully can see that the model fit is not very good\rci.plot(model1) # It should be obvious that the regression line does not reflect the actual relationship well\r\rQuadratic regression 1\rGiven the result just seem with the simple linear regression, we will construct a quadratic model. The structure of the analysis is the same, but we will create a variable for density to reflect the squared term, \\(density^2\\).\n# Define the density as a squared term (there are multiple ways to do this, but we will use a simple approach for now)\rdensity2\u0026lt;-density^2\rmodel2\u0026lt;-lm(yield~density + density2)\r# Significance\ranova(model2)\r## Analysis of Variance Table\r## ## Response: yield\r## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## density 1 43.20 43.200 52.470 1.024e-05 ***\r## density2 1 42.00 42.000 51.012 1.177e-05 ***\r## Residuals 12 9.88 0.823 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rsummary(model2)\r## ## Call:\r## lm(formula = yield ~ density + density2)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -1.50 -0.50 -0.20 0.35 1.80 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 5.80000 1.12359 5.162 0.000236 ***\r## density 0.72000 0.08563 8.409 2.25e-06 ***\r## density2 -0.01000 0.00140 -7.142 1.18e-05 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 0.9074 on 12 degrees of freedom\r## Multiple R-squared: 0.8961, Adjusted R-squared: 0.8788 ## F-statistic: 51.74 on 2 and 12 DF, p-value: 1.259e-06\r# Assumptions\rplot(model2)\r# Let\u0026#39;s focus on comparing the two models based on Cook\u0026#39;s Distance.\rplot(model1, which=4) \rplot(model2, which=4) \r# F-test between model 1 and model 2\ranova(model1, model2)\r## Analysis of Variance Table\r## ## Model 1: yield ~ density\r## Model 2: yield ~ density + density2\r## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 13 51.88 ## 2 12 9.88 1 42 51.012 1.177e-05 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r# Additional tools to understand the model fit and model assumptions for the quadratic form\rinfluence.measures(model2) # this is a general function to create the base for subsequent measurments\r## Influence measures of\r## lm(formula = yield ~ density + density2) :\r## ## dfb.1_ dfb.dnst dfb.dns2 dffit cov.r cook.d hat inf\r## 1 1.46e-01 -0.1121 0.0927 0.1632 1.811 0.00963 0.295 *\r## 2 -4.47e-01 0.3445 -0.2847 -0.5012 1.571 0.08663 0.295 ## 3 2.94e-01 -0.2262 0.1870 0.3292 1.718 0.03850 0.295 ## 4 3.67e-17 -0.0280 0.0373 -0.0849 1.461 0.00261 0.124 ## 5 5.03e-17 -0.1007 0.1339 -0.3054 1.244 0.03199 0.124 ## 6 -3.68e-17 0.0422 -0.0560 0.1278 1.436 0.00588 0.124 ## 7 -5.44e-02 0.0764 -0.0779 0.1016 1.527 0.00373 0.162 ## 8 -6.26e-01 0.8795 -0.8964 1.1687 0.349 0.30236 0.162 ## 9 5.44e-02 -0.0764 0.0779 -0.1016 1.527 0.00373 0.162 ## 10 2.07e-01 -0.2391 0.1976 -0.4506 1.025 0.06529 0.124 ## 11 -1.40e-01 0.1620 -0.1339 0.3054 1.244 0.03199 0.124 ## 12 3.39e-01 -0.3920 0.3240 -0.7388 0.601 0.14691 0.124 ## 13 3.26e-01 -0.4683 0.6225 1.0961 0.919 0.34654 0.295 ## 14 -9.79e-02 0.1406 -0.1870 -0.3292 1.718 0.03850 0.295 ## 15 -4.85e-02 0.0697 -0.0927 -0.1632 1.811 0.00963 0.295 *\rdffits(model2)\r## 1 2 3 4 5 6 ## 0.16317088 -0.50123382 0.32920738 -0.08494387 -0.30538465 0.12778710 ## 7 8 9 10 11 12 ## 0.10156307 1.16874641 -0.10156307 -0.45055886 0.30538465 -0.73883264 ## 13 14 15 ## 1.09610758 -0.32920738 -0.16317088\rdfbeta(model2)\r## (Intercept) density density2\r## 1 1.702703e-01 -0.010000000 1.351351e-04\r## 2 -5.108108e-01 0.030000000 -4.054054e-04\r## 3 3.405405e-01 -0.020000000 2.702703e-04\r## 4 4.299875e-17 -0.002500000 5.434783e-05\r## 5 5.733167e-17 -0.008750000 1.902174e-04\r## 6 -4.299875e-17 0.003750000 -8.152174e-05\r## 7 -6.363636e-02 0.006818182 -1.136364e-04\r## 8 -5.727273e-01 0.061363636 -1.022727e-03\r## 9 6.363636e-02 -0.006818182 1.136364e-04\r## 10 2.282609e-01 -0.020108696 2.717391e-04\r## 11 -1.597826e-01 0.014076087 -1.902174e-04\r## 12 3.423913e-01 -0.030163043 4.076087e-04\r## 13 3.405405e-01 -0.037297297 8.108108e-04\r## 14 -1.135135e-01 0.012432432 -2.702703e-04\r## 15 -5.675676e-02 0.006216216 -1.351351e-04\rcovratio(model2)\r## 1 2 3 4 5 6 7 ## 1.8105775 1.5709359 1.7180496 1.4612786 1.2440860 1.4359881 1.5267335 ## 8 9 10 11 12 13 14 ## 0.3493908 1.5267335 1.0252651 1.2440860 0.6006431 0.9193090 1.7180496 ## 15 ## 1.8105775\rcooks.distance(model2)\r## 1 2 3 4 5 6 ## 0.009626105 0.086634944 0.038504420 0.002611680 0.031993085 0.005876281 ## 7 8 9 10 11 12 ## 0.003732810 0.302357630 0.003732810 0.065292011 0.031993085 0.146907024 ## 13 14 15 ## 0.346539778 0.038504420 0.009626105\rvif(model2) # 26.71 is the value, values greater than 10 typically indicate high collinearity\r## density density2 ## 26.71429 26.71429\r\rQuadratic regression 2\rGiven the result for the first quadratic regression that indicated high collinearity for \\(density\\) and \\(density^2\\), what we can do to remove this without affecting the analysis is to center the density variable and then run the analysis again. This is a very common practice to reduce the impact of not only high collinearity but also for cases for things like multivariate statistics where the scale for individual response variables can have high leverage on the overall analysis. The fuction, scale, allows us to the scale the density considering the mean value (we are not taking into account the variance, which is another common approach = location-scale type centering).\n# Center and standardize the density variable\r# This approach substracts the mean, scale=FALSE tells R that we do not take into account the standard deviation in the analysis\rden_centered\u0026lt;-scale(density, center=TRUE, scale=FALSE) # The same if we did this by \u0026quot;hand\u0026quot;\rdensity-mean(density)\r## [1] -20 -20 -20 -10 -10 -10 0 0 0 10 10 10 20 20 20\r# Create a new variable for density^2 based on centered values\rden_centered2 \u0026lt;- den_centered^2\rplot(den_centered, den_centered2)\r# Regression model with centered data\rmodel3\u0026lt;-lm(yield~den_centered+den_centered2)\ranova(model3)\r## Analysis of Variance Table\r## ## Response: yield\r## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## den_centered 1 43.20 43.200 52.470 1.024e-05 ***\r## den_centered2 1 42.00 42.000 51.012 1.177e-05 ***\r## Residuals 12 9.88 0.823 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rsummary(model3)\r## ## Call:\r## lm(formula = yield ~ den_centered + den_centered2)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -1.50 -0.50 -0.20 0.35 1.80 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 18.40000 0.36511 50.396 2.44e-15 ***\r## den_centered 0.12000 0.01657 7.244 1.02e-05 ***\r## den_centered2 -0.01000 0.00140 -7.142 1.18e-05 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 0.9074 on 12 degrees of freedom\r## Multiple R-squared: 0.8961, Adjusted R-squared: 0.8788 ## F-statistic: 51.74 on 2 and 12 DF, p-value: 1.259e-06\r# Assumptions\rplot(model3)\r# Compare original model with the centered quadratic model\ranova(model1, model3)\r## Analysis of Variance Table\r## ## Model 1: yield ~ density\r## Model 2: yield ~ den_centered + den_centered2\r## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 13 51.88 ## 2 12 9.88 1 42 51.012 1.177e-05 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r# Collinearity?\rdffits(model3)\r## 1 2 3 4 5 6 ## 0.16317088 -0.50123382 0.32920738 -0.08494387 -0.30538465 0.12778710 ## 7 8 9 10 11 12 ## 0.10156307 1.16874641 -0.10156307 -0.45055886 0.30538465 -0.73883264 ## 13 14 15 ## 1.09610758 -0.32920738 -0.16317088\rdfbeta(model3)\r## (Intercept) den_centered den_centered2\r## 1 -0.008108108 -1.891892e-03 1.351351e-04\r## 2 0.024324324 5.675676e-03 -4.054054e-04\r## 3 -0.016216216 -3.783784e-03 2.702703e-04\r## 4 -0.026086957 7.608696e-04 5.434783e-05\r## 5 -0.091304348 2.663043e-03 1.902174e-04\r## 6 0.039130435 -1.141304e-03 -8.152174e-05\r## 7 0.038636364 7.647245e-20 -1.136364e-04\r## 8 0.347727273 9.416246e-19 -1.022727e-03\r## 9 -0.038636364 1.769001e-19 1.136364e-04\r## 10 -0.130434783 -3.804348e-03 2.717391e-04\r## 11 0.091304348 2.663043e-03 -1.902174e-04\r## 12 -0.195652174 -5.706522e-03 4.076087e-04\r## 13 -0.048648649 1.135135e-02 8.108108e-04\r## 14 0.016216216 -3.783784e-03 -2.702703e-04\r## 15 0.008108108 -1.891892e-03 -1.351351e-04\rcovratio(model3)\r## 1 2 3 4 5 6 7 ## 1.8105775 1.5709359 1.7180496 1.4612786 1.2440860 1.4359881 1.5267335 ## 8 9 10 11 12 13 14 ## 0.3493908 1.5267335 1.0252651 1.2440860 0.6006431 0.9193090 1.7180496 ## 15 ## 1.8105775\rcooks.distance(model3)\r## 1 2 3 4 5 6 ## 0.009626105 0.086634944 0.038504420 0.002611680 0.031993085 0.005876281 ## 7 8 9 10 11 12 ## 0.003732810 0.302357630 0.003732810 0.065292011 0.031993085 0.146907024 ## 13 14 15 ## 0.346539778 0.038504420 0.009626105\rvif(model3) #The value is now = 1\r## den_centered den_centered2 ## 1 1\r\rWhat occurred?\rWe will take a look at the correlations between the original forms for density and centered forms.\ncor(density, density2) #high correlation = collinearity\r## [1] 0.9811049\rcor(den_centered, den_centered2) #no correlation\r## [,1]\r## [1,] 0\r\rSummary and considerations\rThe goal of this exercise was to illustrate how one needs to check any package or software regarding assumptions on linear, quadratic, higher polynomial terms. This becomes very important as you consider working with centered or standardized variables.\n\r","date":1564711687,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564711687,"objectID":"57bd9f2a5aa644cb252f874c614a5d2f","permalink":"/post/polynomial_regression/","publishdate":"2019-08-01T21:08:07-05:00","relpermalink":"/post/polynomial_regression/","section":"post","summary":"Background\rIn many studies, for example if one looks the relationship between nitrogen and yield for many cereal crops, the relationship is not linear, rather there is often a plateau where after a specific amount, the response decreases. A simpler linear-type model will explain some of the variability, but not very well. In these situations we can consider a polynomial form to the model.\nWe can define this relationship in general terms as the the relation betweeen the independent variable, \\(x\\), and the expected response, \\(E(y|x)\\).","tags":["Regression"],"title":"Polynomial regression","type":"post"},{"authors":["Paul Esker and Felipe Dalla Lana"],"categories":["R"],"content":"\rBackground\rMany times, we are interested in estimating the relationship between different variables that has a general form described as follows:\n\\[f(x) = E[Y|X=x]\\]\nWhere we do not have a specific function type defined (i.e., specific model):\n\\[Y = f(X) + e\\]\rAs such, we would like to describe the data using the most appropriate model and estimate the parameters. In this introductory exercise, we will use nonparametric methods to do such a task and focus on three possible methods:\n\rMoving average = calculate the mean value, Y, around a window of X values\rWeighted moving averages = kernel smoothing: weight data as a function of distance, i.e., points closer in space are given greater weight\rLocal polynomial regression: adjust the polynomial value based on least squares methods for observations in a local window (weighted by distance)\r\r\rPackages\rlibrary(tidyverse)\rlibrary(Hmisc)\rlibrary(corrplot)\rlibrary(readr)\rlibrary(HH)\rlibrary(car)\rlibrary(scatterplot3d)\rlibrary(leaps)\r\rData\rFor this example, we are using a database called Emissions. This data comes from FAO and represents the amount of \\(CO~2\\) emitted in different countries from Mexico to Panama. The number of years of data collection was 21. The data are also standardized based on the area under agricultural production. Given that one of the authors of this worked in Costa Rica, we will use that as our data source for the exercise. This will required working with a database that is in .csv format and then subset out the part that relates to Costa Rica. To accomplish this first part, we will using coding based on tidyverse, especially using dplyr.\nPlease note: I have located the data in my local Document folder for eash of reading this into R. You can change the location accordingly for your personal use. If you are using this as a script, you can also use the import options in RStudio.\nemissions \u0026lt;- read_csv(\u0026quot;Emissions.csv\u0026quot;)\rhead(emissions)\r## # A tibble: 6 x 5\r## Country Year Area CO2 CO2_area\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Belize 1 128277 7.31 0.000057 ## 2 Belize 2 153923 7.31 0.0000475\r## 3 Belize 3 164124 7.31 0.0000445\r## 4 Belize 4 184274 7.31 0.0000397\r## 5 Belize 5 130610 5.85 0.0000448\r## 6 Belize 6 173667 6.33 0.0000365\r# Quick summary of the results across the countries\rsummaries \u0026lt;- emissions %\u0026gt;% group_by(Country)\rsummaries %\u0026gt;% str()\r## Classes \u0026#39;grouped_df\u0026#39;, \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 168 obs. of 5 variables:\r## $ Country : chr \u0026quot;Belize\u0026quot; \u0026quot;Belize\u0026quot; \u0026quot;Belize\u0026quot; \u0026quot;Belize\u0026quot; ...\r## $ Year : num 1 2 3 4 5 6 7 8 9 10 ...\r## $ Area : num 128277 153923 164124 184274 130610 ...\r## $ CO2 : num 7.31 7.31 7.31 7.31 5.85 ...\r## $ CO2_area: num 5.70e-05 4.75e-05 4.45e-05 3.97e-05 4.48e-05 3.65e-05 2.96e-05 3.36e-05 5.15e-05 5.60e-05 ...\r## - attr(*, \u0026quot;spec\u0026quot;)=\r## .. cols(\r## .. Country = col_character(),\r## .. Year = col_double(),\r## .. Area = col_double(),\r## .. CO2 = col_double(),\r## .. CO2_area = col_double()\r## .. )\r## - attr(*, \u0026quot;groups\u0026quot;)=Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 8 obs. of 2 variables:\r## ..$ Country: chr \u0026quot;Belize\u0026quot; \u0026quot;CostaRica\u0026quot; \u0026quot;ElSalvador\u0026quot; \u0026quot;Guatemala\u0026quot; ...\r## ..$ .rows :List of 8\r## .. ..$ : int 1 2 3 4 5 6 7 8 9 10 ...\r## .. ..$ : int 22 23 24 25 26 27 28 29 30 31 ...\r## .. ..$ : int 43 44 45 46 47 48 49 50 51 52 ...\r## .. ..$ : int 64 65 66 67 68 69 70 71 72 73 ...\r## .. ..$ : int 85 86 87 88 89 90 91 92 93 94 ...\r## .. ..$ : int 106 107 108 109 110 111 112 113 114 115 ...\r## .. ..$ : int 127 128 129 130 131 132 133 134 135 136 ...\r## .. ..$ : int 148 149 150 151 152 153 154 155 156 157 ...\r## ..- attr(*, \u0026quot;.drop\u0026quot;)= logi TRUE\rsummaries %\u0026gt;% summarise(\rem_mean = mean(CO2_area),\rem_sd = sd(CO2_area),\rem_cv = sd(CO2_area)/mean(CO2_area)*100,\rem_max = max(CO2_area),\rem_min = min(CO2_area)\r)\r## # A tibble: 8 x 6\r## Country em_mean em_sd em_cv em_max em_min\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Belize 0.000106 0.000147 139. 0.000668 0.0000296\r## 2 CostaRica 0.000415 0.000100 24.2 0.000649 0.000232 ## 3 ElSalvador 0.000139 0.0000280 20.2 0.000196 0.0000983\r## 4 Guatemala 0.000142 0.0000239 16.8 0.000172 0.000099 ## 5 Honduras 0.000125 0.0000740 59.4 0.000281 0.0000224\r## 6 Mexico 0.000111 0.0000147 13.3 0.000131 0.0000843\r## 7 Nicaragua 0.0000614 0.0000182 29.6 0.0000923 0.0000242\r## 8 Panama 0.000119 0.0000285 23.9 0.000169 0.0000828\r# Create a subset database to work with data only from Costa Rica\rcosta_rica \u0026lt;- filter(emissions, Country==\u0026quot;CostaRica\u0026quot;)\rhead(costa_rica)\r## # A tibble: 6 x 5\r## Country Year Area CO2 CO2_area\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 CostaRica 1 773395 271. 0.00035 ## 2 CostaRica 2 783774 304. 0.000388\r## 3 CostaRica 3 778918 317. 0.000407\r## 4 CostaRica 4 740508 292. 0.000395\r## 5 CostaRica 5 769340 341 0.000443\r## 6 CostaRica 6 765005 341 0.000446\r\rLoess 1\rThis the method based on local polynomial regression.\n# What does the relationship look like?\rCR \u0026lt;- ggplot(data=costa_rica, aes(x=Year, y=CO2_area))\rCR + geom_point()\rCR + geom_point() + geom_line()\r# Loess\rcr_np1 \u0026lt;- with(costa_rica, loess(CO2_area ~ Year , span=0.75)) #default method\rsummary(cr_np1)\r## Call:\r## loess(formula = CO2_area ~ Year, span = 0.75)\r## ## Number of Observations: 21 ## Equivalent Number of Parameters: 4.61 ## Residual Standard Error: 7.132e-05 ## Trace of smoother matrix: 5.06 (exact)\r## ## Control settings:\r## span : 0.75 ## degree : 2 ## family : gaussian\r## surface : interpolate cell = 0.2\r## normalize: TRUE\r## parametric: FALSE\r## drop.square: FALSE\rcrnp1_pred \u0026lt;- predict(cr_np1, data.frame(Year=seq(1,21,0.5)))\rpred1 \u0026lt;- data.frame(Year=seq(1,21,0.5), crnp1_pred)\r# Graphically\rej1 \u0026lt;- ggplot() ej1 +\rgeom_point(data=costa_rica, aes(x=Year, y=CO2_area)) +\rgeom_line(data=costa_rica, aes(x=Year, y=CO2_area), lty=1) +\rgeom_line(data=pred1, aes(x=Year, y=crnp1_pred), lty=2)\r\rModify the Loess line\rLet’s look at some different line forms with Loess.\n# Span=0.5\rcr_np2 \u0026lt;- with(costa_rica, loess(CO2_area ~ Year , span=0.5))\rcrnp2_pred \u0026lt;- predict(cr_np2, data.frame(Year=seq(1,21,0.5)))\rpred2 \u0026lt;- data.frame(Year=seq(1,21,0.5), crnp2_pred)\rej1 \u0026lt;- ggplot() ej1 +\rgeom_point(data=costa_rica, aes(x=Year, y=CO2_area)) +\rgeom_line(data=costa_rica, aes(x=Year, y=CO2_area), lty=1) +\rgeom_line(data=pred1, aes(x=Year, y=crnp1_pred), lty=2, lwd=1.5) +\rgeom_line(data=pred2, aes(x=Year, y=crnp2_pred), lty=3, lwd=1.5)\r# Span=0.25\rcr_np3 \u0026lt;- with(costa_rica, loess(CO2_area ~ Year, span=0.25))\rcrnp3_pred \u0026lt;- predict(cr_np3, data.frame(Year=seq(1,21,0.5)))\rpred3 \u0026lt;- data.frame(Year=seq(1,21,0.5), crnp2_pred)\rej1 \u0026lt;- ggplot() ej1 +\rgeom_point(data=costa_rica, aes(x=Year, y=CO2_area)) +\rgeom_line(data=costa_rica, aes(x=Year, y=CO2_area), lty=1) +\rgeom_line(data=pred1, aes(x=Year, y=crnp1_pred), lty=2, lwd=1.5) +\rgeom_line(data=pred2, aes(x=Year, y=crnp2_pred), lty=3, lwd=1.5) +\rgeom_line(data=pred3, aes(x=Year, y=crnp3_pred), lty=4, lwd=1.5)\r\rSmoothing splines\rIn our next example, we will use the function smooth.spline(). With this method, we can change the smoothing parameter and the methodology is based on crossed-validation to be able to define the parameter.\n# Base method (by default)\rcr_spline \u0026lt;- with(costa_rica, smooth.spline(x=Year, y=CO2_area))\rcr_spline\r## Call:\r## smooth.spline(x = Year, y = CO2_area)\r## ## Smoothing Parameter spar= 0.3976519 lambda= 6.497957e-05 (11 iterations)\r## Equivalent Degrees of Freedom (Df): 9.578523\r## Penalized Criterion (RSS): 2.323599e-08\r## GCV: 3.740554e-09\rsummary(cr_spline)\r## Length Class Mode ## x 21 -none- numeric\r## y 21 -none- numeric\r## w 21 -none- numeric\r## yin 21 -none- numeric\r## tol 1 -none- numeric\r## data 3 -none- list ## no.weights 1 -none- logical\r## lev 21 -none- numeric\r## cv.crit 1 -none- numeric\r## pen.crit 1 -none- numeric\r## crit 1 -none- numeric\r## df 1 -none- numeric\r## spar 1 -none- numeric\r## ratio 1 -none- numeric\r## lambda 1 -none- numeric\r## iparms 5 -none- numeric\r## auxM 0 -none- NULL ## fit 5 smooth.spline.fit list ## call 3 -none- call\rcrsp_pred \u0026lt;- predict(cr_spline, data.frame(Year=seq(1,21,0.5)))\rpred4 \u0026lt;- data.frame(Year=seq(1,21,0.5), crsp_pred)\r#Compare the fit with the Loess fit\rej1 \u0026lt;- ggplot() ej1 +\rgeom_point(data=costa_rica, aes(x=Year, y=CO2_area)) +\rgeom_line(data=costa_rica, aes(x=Year, y=CO2_area), lty=1) +\rgeom_line(data=pred1, aes(x=Year, y=crnp1_pred), lty=2, lwd=1.5) +\rgeom_line(data=pred4, aes(x=Year, y=Year.2), lty=4, lwd=1.5)\r\rChange smoothing parameter\rWe will now create a series of model runs where we change the smoothing parameter.\ncr25 \u0026lt;- with(costa_rica, smooth.spline(x=Year, y=CO2_area, spar=0.25))\rpred25 \u0026lt;- data.frame(Year=seq(1,21,0.5), pred=(predict(cr25, data.frame(Year=seq(1,21,0.5)))))\rcr35 \u0026lt;- with(costa_rica, smooth.spline(x=Year, y=CO2_area, spar=0.35))\rpred35 \u0026lt;- data.frame(Year=seq(1,21,0.5), pred=(predict(cr35, data.frame(Year=seq(1,21,0.5)))))\rcr45 \u0026lt;- with(costa_rica, smooth.spline(x=Year, y=CO2_area, spar=0.45))\rpred45 \u0026lt;- data.frame(Year=seq(1,21,0.5), pred=(predict(cr45, data.frame(Year=seq(1,21,0.5)))))\rcr55 \u0026lt;- with(costa_rica, smooth.spline(x=Year, y=CO2_area, spar=0.55))\rpred55 \u0026lt;- data.frame(Year=seq(1,21,0.5), pred=(predict(cr55, data.frame(Year=seq(1,21,0.5)))))\rcr65 \u0026lt;- with(costa_rica, smooth.spline(x=Year, y=CO2_area, spar=0.65))\rpred65 \u0026lt;- data.frame(Year=seq(1,21,0.5), pred=(predict(cr65, data.frame(Year=seq(1,21,0.5)))))\rcr75 \u0026lt;- with(costa_rica, smooth.spline(x=Year, y=CO2_area, spar=0.75))\rpred75 \u0026lt;- data.frame(Year=seq(1,21,0.5), pred=(predict(cr75, data.frame(Year=seq(1,21,0.5)))))\rcr85 \u0026lt;- with(costa_rica, smooth.spline(x=Year, y=CO2_area, spar=0.85))\rpred85 \u0026lt;- data.frame(Year=seq(1,21,0.5), pred=(predict(cr85, data.frame(Year=seq(1,21,0.5)))))\rej1 \u0026lt;- ggplot() ej1 +\rgeom_point(data=costa_rica, aes(x=Year, y=CO2_area)) +\rgeom_line(data=costa_rica, aes(x=Year, y=CO2_area), lty=1) +\rgeom_line(data=pred25, aes(x=Year, y=pred.Year.1), lty=2, lwd=1.2) +\rgeom_line(data=pred35, aes(x=Year, y=pred.Year.1), lty=3, lwd=1.2) +\rgeom_line(data=pred45, aes(x=Year, y=pred.Year.1), lty=4, lwd=1.2) +\rgeom_line(data=pred55, aes(x=Year, y=pred.Year.1), lty=5, lwd=1.2) +\rgeom_line(data=pred65, aes(x=Year, y=pred.Year.1), lty=6, lwd=1.2) +\rgeom_line(data=pred75, aes(x=Year, y=pred.Year.1), lty=2, lwd=1.3) +\rgeom_line(data=pred85, aes(x=Year, y=pred.Year.1), lty=3, lwd=1.3) \r\rLast word for now\rTo close this discussion, it is natural to ask the following question, “What methods can we use to examine and control the smoothing parameter?”\nWithin this list, there are several including:\n\rtrial and error,\rdegree of smoothing compared with the data fidelity or reliability,\rminimize the mean square error,\ruse cross-validation methods.\r\r\r","date":1564711686,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564711686,"objectID":"01d41bf37a7a2d5b3db7827e51b7a4a9","permalink":"/post/nonlinear_regression/","publishdate":"2019-08-01T21:08:06-05:00","relpermalink":"/post/nonlinear_regression/","section":"post","summary":"Background\rMany times, we are interested in estimating the relationship between different variables that has a general form described as follows:\n\\[f(x) = E[Y|X=x]\\]\nWhere we do not have a specific function type defined (i.e., specific model):\n\\[Y = f(X) + e\\]\rAs such, we would like to describe the data using the most appropriate model and estimate the parameters. In this introductory exercise, we will use nonparametric methods to do such a task and focus on three possible methods:","tags":["Regression"],"title":"Nonlinear regression","type":"post"},{"authors":["Paul Esker and Felipe Dalla Lana"],"categories":["R"],"content":"\rBackground\rNonlinear regression is an important modeling tool for looking at more compliated biological, physiological, etc., relationships. This introductory exercise describes some of the concepts that one should consider when analyzing nonlinear data. The process is iterative for modeling fitting, meaning that the parameters are estimated in a stepwise fashion. In Plant Pathology this is a useful tool for things like disease development over time. These models can be further extended to incorporated additional factors like treatments, years, among other things, to study the overall behavior and observed variability.\n# Note that for this example, we will keep the tools to those that are available in the base package\rlibrary(tidyverse)\r\rData\rThis work originated in Costa Rica and focused on growth and development of onion in the northern areas of the Province of Cartago. Growth was measured using whole plant biomass. The goal was to understand how different varieties performed in this zone and future work would examine the impact of different management tactics and pests on improving overall productivity.\nThe data strcuture for the orginal worked involved three cultivars but for the exercise we will only focus on one of those, which is called Alvara.\n\rdap = days after planting\rgdd = growing degree days based on threshold temperatures for onion\rroot = root dry weight (grams)\rbuld = bulb dry weight (grams)\raerial = aerial biomass dry weight (grams)\rtotal = total dry weight considering the above measurements\r\rdap \u0026lt;- c(11, 18, 26, 33, 40, 47, 56, 61, 69, 82, 96, 111, 124)\rgdd \u0026lt;- c(148, 233, 327, 410, 492, 575, 686, 746, 837, 993, 1158, 1335, 1484)\rroot \u0026lt;- c(0.04, 0.019, 0.113, 0.044, 0.045, 0.056, 0.08, 0.114, 0.109, 0.116, 0.098, 0.101, 0.066)\rbulb \u0026lt;- c(0.137, 0.166, 0.289, 0.2, 0.292, 0.298, 0.474, 0.416, 1.236, 2.594, 6.265, 6.174, 22.521)\raerial \u0026lt;- c(0.162, 0.191, 0.308, 0.243, 0.25, 0.343, 0.988, 0.962, 2.593, 3.379, 2.83, 5.054,\r2.748)\rtotal \u0026lt;- c(0.34, 0.376, 0.711, 0.487, 0.587, 0.698, 1.542, 1.492, 3.938, 6.089, 9.193, 11.329,\r25.334)\ralvara \u0026lt;- data.frame(dap, gdd, root, bulb, aerial, total)\r\rGraphical representation - preliminary\rGraphically, we will use some basic tools to look at the overall behavior.\n## The predictive factor is growing degree days, assuming that best explains the overall growth and development\rwith(alvara, plot(x=gdd, y=root, type=\u0026quot;b\u0026quot;, lty=1, lwd=2, pch=19, col=\u0026quot;black\u0026quot;))\rwith(alvara, plot(x=gdd, y=bulb, type=\u0026quot;b\u0026quot;, lty=1, lwd=2, pch=19, col=\u0026quot;black\u0026quot;))\rwith(alvara, plot(x=gdd, y=aerial, type=\u0026quot;b\u0026quot;, lty=1, lwd=2, pch=19, col=\u0026quot;black\u0026quot;))\rwith(alvara, plot(x=gdd, y=total, type=\u0026quot;b\u0026quot;, lty=1, lwd=2, pch=19, col=\u0026quot;black\u0026quot;))\r\rModeling\rSelecting a nonlinear model depends on many factors especially taking into account the biological relationship. In preliminary analyses with data of this type for onion, we saw two possible nonlinear models that best describe the relationships. In the first case the development was bell-shaped, showing increases until a specific point in the developmental process when dry weight was reduced. The second curve tupe was exponential and related to the ultimate growth phases just prior to harvest. Even though growth is not infinite, we still recognize that this model may explain well the relationship and is interpretable.\nModel 1.\n\\[ DW = \\alpha * exp(-\\beta * (gdd-\\gamma)^2)\\]\nWhere, DW is the dry weight (g), \\(\\alpha\\) is the measure of initial dry weight the start of evaluations (0 gdds), \\(\\beta\\) is the growth rate, and \\(\\gamma\\) represents the inflexion point in the process. “gdd” are the accumulated growing degree days.\nModel 2.\n\\[ DW = X_0 * exp (K * gdd) \\]\nwhere, DW is the dry weight (g), \\(X_0\\) is the condition where there has been no accumulation of heat units, and K is the growth rate as a function of the accumulated growing degree days (gdd).\nIn both cases, it is important to take into account that the model will be adjusted based on the definition of the initial starting parameters. There are different methods to define initial starting parameters, including:\n\rusing a grid search approach to find the best combination of all parameters in the model\rusing preliminary analyses to define the parameters (can be based on similar data to your situation)\rfunctional estimate based on the model form and your knowledge about the system\rgenetic algorithms, see for example, https://en.wikipedia.org/wiki/Genetic_algorithm\rin R there are also for some of the models there are functions that will obtain initial starting parameters (see: http://www.apsnet.org/edcenter/advanced/topics/EcologyAndEpidemiologyInR/DiseaseProgress/Pages/NonlinearRegression.aspx)\r\rFor the following examples, we will use the third method based on knowledge of the crop physiology and preliminary analyses.\n\rModel 1\r## Variable = root dry weight\r## nls = nonlinear least squares\r## start=list() provides the input to define the initial starting values for the parameters\rregnl1 \u0026lt;- nls(root ~ alpha * exp(-beta*(gdd-gamma)^2), start=list(alpha = 0.15, beta = 0.0000002, gamma = 900), trace=TRUE, data=alvara)\r## 0.07027501 : 1.5e-01 2.0e-07 9.0e+02\r## 0.008673095 : 1.007217e-01 7.854471e-07 1.308727e+03\r## 0.007106798 : 9.761952e-02 1.203522e-06 1.002286e+03\r## 0.006594237 : 1.069288e-01 1.562084e-06 1.021834e+03\r## 0.006588507 : 1.079002e-01 1.618805e-06 1.016275e+03\r## 0.006588451 : 1.079549e-01 1.621248e-06 1.016766e+03\r## 0.00658845 : 1.079619e-01 1.621827e-06 1.016741e+03\r## 0.00658845 : 1.079626e-01 1.621871e-06 1.016743e+03\rsummary(regnl1)\r## ## Formula: root ~ alpha * exp(-beta * (gdd - gamma)^2)\r## ## Parameters:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## alpha 1.080e-01 1.268e-02 8.516 6.78e-06 ***\r## beta 1.622e-06 7.130e-07 2.275 0.0462 * ## gamma 1.017e+03 9.913e+01 10.257 1.26e-06 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 0.02567 on 10 degrees of freedom\r## ## Number of iterations to convergence: 7 ## Achieved convergence tolerance: 2.89e-06\r## Predictions\rregnl_pred \u0026lt;- predict(regnl1, data.frame(gdd=seq(100,1500,25)))\r## Database of predictions over a range of gdds\rpredictions \u0026lt;- data.frame(gdd=seq(100,1500,25), pred=regnl_pred)\r## Graphically\rej1 \u0026lt;- ggplot() ej1 +\rgeom_point(data=alvara, aes(x=gdd, y=root)) +\rgeom_line(data=predictions, aes(x=gdd, y=pred), lty=1, lwd=1.5)\r\rModel 2\r## Variable = total dry weight regnl2 \u0026lt;- nls(total ~ x0 * exp(k * gdd), start = list(x0=0.5, k=0.0002), trace=TRUE, data=alvara)\r## 838.571 : 5e-01 2e-04\r## 745.1133 : 0.164848298 0.001679615\r## 547.7136 : 0.066722470 0.002956091\r## 231.1729 : 0.086874301 0.003337606\r## 66.82676 : 0.124641432 0.003367594\r## 17.75713 : 0.161921536 0.003372197\r## 17.75511 : 0.16183068 0.00337153\r## 17.75511 : 0.16184411 0.00337147\rsummary(regnl2)\r## ## Formula: total ~ x0 * exp(k * gdd)\r## ## Parameters:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## x0 0.1618441 0.0645079 2.509 0.029 * ## k 0.0033715 0.0002832 11.905 1.26e-07 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 1.27 on 11 degrees of freedom\r## ## Number of iterations to convergence: 7 ## Achieved convergence tolerance: 5.642e-06\r## Predictions\rregn2_pred \u0026lt;- predict(regnl2, data.frame(gdd=seq(100,1500,25)))\r## Database of predictions\rpredictions \u0026lt;- data.frame(gdd=seq(100,1500,25), pred=regn2_pred)\r## Graphically\rej2 \u0026lt;- ggplot() ej2 +\rgeom_point(data=alvara, aes(x=gdd, y=total)) +\rgeom_line(data=predictions, aes(x=gdd, y=pred), lty=1, lwd=1.5)\r\rExercises\rPerform the same set of the analyses for the following measurements and initial parameter conditions:\nAerial dry weight, you can consider the following starting parameter values: start=list(alpha = 5, beta = 0.00002, gamma = 1100).\nBulb dry weight, you can consider the following starting parameter values: start=list(x0 = 0.5, k = 0.0002).\n\r","date":1564711685,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564711685,"objectID":"dd89304ffad45deaff19aeccc2321329","permalink":"/post/nonparametric_regression/","publishdate":"2019-08-01T21:08:05-05:00","relpermalink":"/post/nonparametric_regression/","section":"post","summary":"Background\rNonlinear regression is an important modeling tool for looking at more compliated biological, physiological, etc., relationships. This introductory exercise describes some of the concepts that one should consider when analyzing nonlinear data. The process is iterative for modeling fitting, meaning that the parameters are estimated in a stepwise fashion. In Plant Pathology this is a useful tool for things like disease development over time. These models can be further extended to incorporated additional factors like treatments, years, among other things, to study the overall behavior and observed variability.","tags":["Regression"],"title":"Nonparametric regression","type":"post"}]